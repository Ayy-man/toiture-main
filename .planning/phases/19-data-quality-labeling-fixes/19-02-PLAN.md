---
phase: 19-data-quality-labeling-fixes
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/schemas/dashboard.py
  - backend/app/routers/dashboard.py
autonomous: false

must_haves:
  truths:
    - "2022 labor records have data_quality_flag = 'labor_unreliable_2022' in Supabase estimates table"
    - "Flagged 2022 data is excluded from any future training pipeline"
    - "Estimates table has data_quality_flag TEXT column and created_by TEXT column"
    - "Backend /dashboard/compliance endpoint returns sqft completion rate per estimator"
  artifacts:
    - path: "backend/app/schemas/dashboard.py"
      provides: "ComplianceReport and EstimatorCompliance Pydantic models"
      contains: "ComplianceReport"
    - path: "backend/app/routers/dashboard.py"
      provides: "GET /dashboard/compliance endpoint"
      contains: "compliance"
  key_links:
    - from: "backend/app/routers/dashboard.py"
      to: "backend/app/schemas/dashboard.py"
      via: "ComplianceReport response model import"
      pattern: "ComplianceReport"
---

<objective>
Flag 1,512 corrupted 2022 labor quotes in the database, add the created_by column for estimator tracking, and build the backend compliance endpoint. This is the database + backend foundation that Plan 19-03 (frontend) depends on.

Purpose: Laurent identified 2022 labor cost data as unreliable. Flagging it prevents polluted training data. The created_by column enables per-estimator compliance tracking for sqft data entry.

Output: Database migration SQL, backend compliance endpoint, and documentation for training pipeline exclusion.
</objective>

<execution_context>
@/Users/aymanbaig/.claude/get-shit-done/workflows/execute-plan.md
@/Users/aymanbaig/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@backend/app/schemas/dashboard.py
@backend/app/routers/dashboard.py
@backend/app/schemas/estimate.py
@backend/app/schemas/hybrid_quote.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create SQL migration and add backend compliance schemas + endpoint</name>
  <files>
    backend/app/schemas/dashboard.py
    backend/app/routers/dashboard.py
  </files>
  <action>
**Part A — SQL Migration Script:**

Create file `backend/migrations/019_data_quality_flags.sql` with:

```sql
-- Phase 19: Data Quality & Labeling Fixes
-- Add data quality flag column for marking unreliable records
ALTER TABLE estimates ADD COLUMN IF NOT EXISTS data_quality_flag TEXT NULL;

-- Add estimator tracking column for compliance monitoring
ALTER TABLE estimates ADD COLUMN IF NOT EXISTS created_by TEXT NULL;

-- Flag 2022 labor data as unreliable (1,512 records)
UPDATE estimates
SET data_quality_flag = 'labor_unreliable_2022'
WHERE EXTRACT(YEAR FROM created_at) = 2022
  AND data_quality_flag IS NULL;

-- Create indexes for compliance queries
CREATE INDEX IF NOT EXISTS idx_estimates_created_by ON estimates(created_by);
CREATE INDEX IF NOT EXISTS idx_estimates_quality_flag ON estimates(data_quality_flag);
```

NOTE: This SQL must be run manually against Supabase. It cannot be automated because Supabase requires Dashboard/SQL Editor access. The executor should include this SQL in the SUMMARY.md for the user to run.

**Part B — Backend Schemas (`backend/app/schemas/dashboard.py`):**

Add two new Pydantic models at the end of the file (after `DashboardCharts`):

```python
class EstimatorCompliance(BaseModel):
    """Sqft completion rate for a single estimator."""
    name: str
    total_estimates: int
    sqft_completed: int
    completion_rate: float  # 0.0 to 1.0


class ComplianceReport(BaseModel):
    """Compliance report for sqft data entry tracking."""
    overall_completion_rate: float  # 0.0 to 1.0
    estimators: List[EstimatorCompliance]
    alert: bool  # True if overall rate < 80%
    total_estimates: int
    total_with_sqft: int
```

**Part C — Backend Compliance Endpoint (`backend/app/routers/dashboard.py`):**

Add a new endpoint at the end of the file:

```python
@router.get("/compliance", response_model=ComplianceReport)
def get_compliance_report(
    days: int = Query(default=30, ge=1, le=365, description="Number of days to look back"),
):
    """Get sqft data entry compliance report.

    Returns overall and per-estimator sqft completion rates.
    Alert triggers if overall rate drops below 80%.
    """
    supabase = get_supabase()
    if supabase is None:
        raise HTTPException(
            status_code=503, detail="Supabase not configured. Compliance unavailable."
        )

    try:
        from datetime import datetime, timedelta
        cutoff = (datetime.utcnow() - timedelta(days=days)).isoformat()

        # Fetch estimates within time window
        # Select only needed columns: sqft, created_by, category
        result = supabase.table("estimates").select(
            "sqft, created_by, category"
        ).gte("created_at", cutoff).execute()

        rows = result.data

        # Exclude Service Call category from sqft requirement
        non_service_rows = [r for r in rows if r.get("category") != "Service Call"]

        total = len(non_service_rows)
        with_sqft = sum(1 for r in non_service_rows if r.get("sqft") and r["sqft"] > 0)
        overall_rate = with_sqft / total if total > 0 else 1.0

        # Per-estimator breakdown
        estimator_data: dict = {}
        for row in non_service_rows:
            name = row.get("created_by") or "Unknown"
            if name not in estimator_data:
                estimator_data[name] = {"total": 0, "with_sqft": 0}
            estimator_data[name]["total"] += 1
            if row.get("sqft") and row["sqft"] > 0:
                estimator_data[name]["with_sqft"] += 1

        estimators = [
            EstimatorCompliance(
                name=name,
                total_estimates=data["total"],
                sqft_completed=data["with_sqft"],
                completion_rate=data["with_sqft"] / data["total"] if data["total"] > 0 else 1.0,
            )
            for name, data in sorted(estimator_data.items())
        ]

        return ComplianceReport(
            overall_completion_rate=overall_rate,
            estimators=estimators,
            alert=overall_rate < 0.8,
            total_estimates=total,
            total_with_sqft=with_sqft,
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to fetch compliance report: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch compliance report")
```

Import `EstimatorCompliance` and `ComplianceReport` from `app.schemas.dashboard` at the top of the file (add to existing import block).

Also import `Query` is already imported. Add `from datetime import datetime, timedelta` at the module top-level imports (not inside the function — move it out).
  </action>
  <verify>
1. Check schemas: `cd backend && python -c "from app.schemas.dashboard import ComplianceReport, EstimatorCompliance; print('OK')"` — should print OK.
2. Check router imports: `cd backend && python -c "from app.routers.dashboard import router; print([r.path for r in router.routes])"` — should include "/compliance".
3. Verify migration file exists: `ls backend/migrations/019_data_quality_flags.sql`.
  </verify>
  <done>
SQL migration script created. ComplianceReport and EstimatorCompliance schemas added. GET /dashboard/compliance endpoint returns per-estimator sqft completion rates with alert flag.
  </done>
</task>

<task type="checkpoint:human-action" gate="blocking">
  <name>Task 2: Run SQL migration in Supabase</name>
  <files>backend/migrations/019_data_quality_flags.sql</files>
  <action>
This is a human-action checkpoint. The user must execute the SQL migration manually in Supabase.

SQL migration script was created at `backend/migrations/019_data_quality_flags.sql` in Task 1.

Steps for user:
1. Open Supabase Dashboard → SQL Editor
2. Copy the contents of `backend/migrations/019_data_quality_flags.sql`
3. Run the SQL
4. Verify: Run `SELECT COUNT(*) FROM estimates WHERE data_quality_flag = 'labor_unreliable_2022';` — should return ~1,512
5. Verify: Run `SELECT column_name FROM information_schema.columns WHERE table_name = 'estimates' AND column_name IN ('data_quality_flag', 'created_by');` — should return 2 rows

Resume signal: Type "migration done" when the SQL has been executed in Supabase.
  </action>
  <verify>User confirms the migration was run successfully in Supabase.</verify>
  <done>Supabase estimates table has data_quality_flag and created_by columns. 2022 records flagged with 'labor_unreliable_2022'.</done>
</task>

</tasks>

<verification>
1. Backend starts without import errors
2. `GET /dashboard/compliance?days=30` returns valid JSON with overall_completion_rate, estimators array, and alert boolean
3. SQL migration adds 2 columns and flags 2022 data
4. Training pipeline documentation updated (in SUMMARY.md)
</verification>

<success_criteria>
- `data_quality_flag` and `created_by` columns exist in estimates table
- ~1,512 records flagged with 'labor_unreliable_2022'
- GET /dashboard/compliance endpoint functional
- ComplianceReport schema validates correctly
</success_criteria>

<output>
After completion, create `.planning/phases/19-data-quality-labeling-fixes/19-02-SUMMARY.md`

IMPORTANT: Include in SUMMARY.md a "Training Pipeline Note" section:
- 2022 labor data is now flagged with `data_quality_flag = 'labor_unreliable_2022'`
- Future training runs in `/cortex-data/train_cortex_v4.py` MUST filter: `WHERE data_quality_flag IS NULL`
- This ensures corrupted 2022 labor quotes are excluded from model training
</output>
