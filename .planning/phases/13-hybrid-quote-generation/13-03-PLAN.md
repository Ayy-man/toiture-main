---
phase: 13-hybrid-quote-generation
plan: 03
type: execute
wave: 3
depends_on: ["13-01", "13-02"]
files_modified:
  - backend/app/services/hybrid_quote.py
  - backend/app/config.py
autonomous: true

must_haves:
  truths:
    - "CBR and ML predictions run in parallel using asyncio.gather()"
    - "LLM merges outputs using Anthropic structured outputs with Pydantic"
    - "Fallback works when CBR fails (ML-only) or ML fails (CBR-only)"
    - "Response time target is <5 seconds for full quote"
  artifacts:
    - path: "backend/app/services/hybrid_quote.py"
      provides: "Orchestration layer for CBR + ML + LLM merger"
      min_lines: 200
      exports: ["generate_hybrid_quote", "init_anthropic_client", "close_anthropic_client"]
    - path: "backend/app/config.py"
      provides: "Anthropic API key configuration"
      contains: "anthropic_api_key"
  key_links:
    - from: "backend/app/services/hybrid_quote.py"
      to: "backend/app/services/pinecone_cbr.py"
      via: "query_similar_cases import"
      pattern: "from app.services.pinecone_cbr import"
    - from: "backend/app/services/hybrid_quote.py"
      to: "backend/app/services/material_predictor.py"
      via: "predict_materials import"
      pattern: "from app.services.material_predictor import"
    - from: "backend/app/services/hybrid_quote.py"
      to: "anthropic.Anthropic"
      via: "client.messages.create"
      pattern: "client\\.messages\\.create"
---

<objective>
Create the hybrid quote orchestrator service that runs CBR and ML predictions in parallel, then uses Anthropic's Claude Sonnet 4.5 with structured outputs to merge results into a final quote with three-tier pricing.

Purpose: This is the core intelligence of Phase 13 - coordinating three AI systems (CBR retrieval, ML prediction, LLM reasoning) into a unified quote generation pipeline. The orchestrator handles partial failures gracefully and respects the <5 second SLA.

Output: backend/app/services/hybrid_quote.py with async orchestration + LLM merger
</objective>

<execution_context>
@/Users/aymanbaig/.claude/get-shit-done/workflows/execute-plan.md
@/Users/aymanbaig/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/13-hybrid-quote-generation/13-CONTEXT.md
@.planning/phases/13-hybrid-quote-generation/13-RESEARCH.md
@.planning/phases/13-hybrid-quote-generation/13-01-SUMMARY.md
@.planning/phases/13-hybrid-quote-generation/13-02-SUMMARY.md

# Existing services to orchestrate
@backend/app/services/predictor.py
@backend/app/services/material_predictor.py
@backend/app/services/pinecone_cbr.py
@backend/app/services/embeddings.py
@backend/app/services/llm_reasoning.py
@backend/app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Anthropic API key to config</name>
  <files>backend/app/config.py</files>
  <action>
Add Anthropic API key setting to the Settings class:

```python
# Anthropic settings (hybrid quote LLM merger)
anthropic_api_key: str = ""
anthropic_model: str = "claude-sonnet-4-5-20250514"
```

This allows using Anthropic's structured outputs API directly (required for tool calling with Pydantic schemas). The OpenRouter client doesn't support structured outputs.

Note: Add after the existing OpenRouter settings block.
</action>
  <verify>
`cd /Users/aymanbaig/Desktop/Toiture-P1/backend && python -c "from app.config import settings; print(f'anthropic_api_key configured: {bool(settings.anthropic_api_key)}')"` should show the setting exists (may be empty string if not set in .env)
</verify>
  <done>
Settings class includes anthropic_api_key and anthropic_model fields
</done>
</task>

<task type="auto">
  <name>Task 2: Create hybrid quote orchestrator service</name>
  <files>backend/app/services/hybrid_quote.py</files>
  <action>
Create the main orchestration service following patterns from RESEARCH.md:

```python
"""Hybrid quote generation orchestrator.

Coordinates CBR (similar case retrieval) and ML (material/labor prediction)
in parallel, then uses Anthropic Claude to merge results into final quote.

Architecture:
1. Parallel: CBR query + ML prediction (asyncio.gather)
2. Sequential: LLM merger with structured outputs
3. Confidence scoring from combined signals

Response time target: <5 seconds
"""

import asyncio
import logging
import time
from typing import Dict, Any, List, Optional, Tuple

from anthropic import Anthropic

from app.config import settings
from app.schemas.hybrid_quote import (
    HybridQuoteRequest,
    HybridQuoteResponse,
    HybridQuoteOutput,
    PricingTier,
)
from app.services.confidence_scorer import (
    calculate_confidence,
    calculate_confidence_ml_only,
    calculate_data_completeness,
)
from app.services.embeddings import build_query_text, generate_query_embedding
from app.services.material_predictor import predict_materials
from app.services.pinecone_cbr import query_similar_cases
from app.services.predictor import predict

logger = logging.getLogger(__name__)

# Module-level Anthropic client
_anthropic_client: Optional[Anthropic] = None


def init_anthropic_client() -> None:
    """Initialize Anthropic client for LLM merger. Called from lifespan."""
    global _anthropic_client
    if not settings.anthropic_api_key:
        logger.warning("Anthropic API key not set, LLM merger will be unavailable")
        return
    logger.info("Initializing Anthropic client for hybrid quote...")
    _anthropic_client = Anthropic(api_key=settings.anthropic_api_key, timeout=30.0)
    logger.info("Anthropic client initialized")


def close_anthropic_client() -> None:
    """Cleanup on shutdown."""
    global _anthropic_client
    _anthropic_client = None
    logger.info("Anthropic client closed")


def get_anthropic_client() -> Anthropic:
    """Get Anthropic client, raising if not initialized."""
    if _anthropic_client is None:
        raise RuntimeError("Anthropic client not initialized. Set ANTHROPIC_API_KEY.")
    return _anthropic_client


async def _run_cbr_query(request: HybridQuoteRequest) -> List[Dict[str, Any]]:
    """Run CBR query in async context."""
    loop = asyncio.get_event_loop()

    def sync_cbr():
        query_text = build_query_text(
            sqft=request.sqft,
            category=request.category,
            complexity=request.complexity_aggregate,
            material_lines=request.material_lines,
            labor_lines=request.labor_lines,
        )
        query_vector = generate_query_embedding(query_text)
        return query_similar_cases(
            query_vector=query_vector,
            top_k=5,
            category_filter=None,  # Let similarity decide
        )

    return await loop.run_in_executor(None, sync_cbr)


async def _run_ml_prediction(request: HybridQuoteRequest) -> Dict[str, Any]:
    """Run ML prediction in async context."""
    loop = asyncio.get_event_loop()

    def sync_ml():
        # Get price prediction
        price_result = predict(
            sqft=request.sqft,
            category=request.category,
            material_lines=request.material_lines,
            labor_lines=request.labor_lines,
            has_subs=1 if request.has_subs else 0,
            complexity=request.complexity_aggregate,
        )

        # Get material prediction
        material_result = predict_materials(
            sqft=request.sqft,
            category=request.category,
            complexity=request.complexity_aggregate,
            has_chimney=request.has_chimney,
            has_skylights=request.has_skylights,
            material_lines=request.material_lines,
            labor_lines=request.labor_lines,
            has_subs=request.has_subs,
            quoted_total=request.quoted_total,
        )

        return {
            "price": price_result,
            "materials": material_result,
        }

    return await loop.run_in_executor(None, sync_ml)


def _format_merger_prompt(
    request: HybridQuoteRequest,
    ml_result: Dict[str, Any],
    cbr_cases: List[Dict[str, Any]],
) -> str:
    """Format prompt for LLM to merge CBR + ML predictions."""

    # Format CBR cases
    cbr_summary = ""
    if cbr_cases:
        for i, case in enumerate(cbr_cases[:5], 1):
            cbr_summary += f"{i}. {case.get('category', 'N/A')}"
            if case.get('sqft'):
                cbr_summary += f", {case['sqft']:,.0f} sqft"
            if case.get('total'):
                cbr_summary += f", ${case['total']:,.0f}"
            cbr_summary += f" ({case.get('similarity', 0):.0%} similar)\n"
    else:
        cbr_summary = "No similar cases found (ML-only mode)\n"

    # Format ML materials (top 10 for brevity)
    ml_materials = ml_result.get("materials", {}).get("materials", [])[:10]
    ml_summary = ""
    for m in ml_materials:
        ml_summary += f"- ID {m['material_id']}: qty {m['quantity']:.1f}, ${m['total']:.0f}\n"

    ml_price = ml_result.get("price", {})

    prompt = f"""You are merging CBR (case-based) and ML (model-based) predictions into a final roofing quote.

**JOB DETAILS:**
Category: {request.category}
Square Footage: {request.sqft:,.0f}
Complexity Score: {request.complexity_aggregate}/56

6-Factor Breakdown:
- Access Difficulty: {request.access_difficulty}/10
- Roof Pitch: {request.roof_pitch}/8
- Penetrations: {request.penetrations}/10
- Material Removal: {request.material_removal}/8
- Safety Concerns: {request.safety_concerns}/10
- Timeline Constraints: {request.timeline_constraints}/10

Has Chimney: {request.has_chimney}
Has Skylights: {request.has_skylights}

**CBR RESULTS (5 most similar historical jobs):**
{cbr_summary}

**ML PREDICTIONS:**
Price Estimate: ${ml_price.get('estimate', 0):,.0f} ({ml_price.get('confidence', 'N/A')} confidence)
Range: ${ml_price.get('range_low', 0):,.0f} - ${ml_price.get('range_high', 0):,.0f}

Top 10 Predicted Materials:
{ml_summary}

Total Materials Cost: ${ml_result.get('materials', {}).get('total_materials_cost', 0):,.0f}

**MERGER RULES:**
1. For materials appearing in 3+/5 CBR cases, trust CBR quantities
2. For other materials, average CBR and ML quantities
3. Work items: prefer CBR (91% coverage) when available
4. Generate 3 pricing tiers:
   - Basic: Essential materials only, standard labor
   - Standard: Full material list, standard labor
   - Premium: Premium materials + expedited labor (+15-20%)

**OUTPUT REQUIREMENTS:**
- List all work items with labor hours
- List all materials with IDs, quantities, and costs
- Calculate total_labor_hours, total_materials_cost, total_price
- Set overall_confidence (0-1) based on CBR/ML agreement
- Provide brief reasoning for key merger decisions
- Generate exactly 3 pricing tiers (Basic, Standard, Premium)

Be accurate. This quote will be reviewed by Laurent."""

    return prompt


async def _merge_with_llm(
    request: HybridQuoteRequest,
    ml_result: Dict[str, Any],
    cbr_cases: List[Dict[str, Any]],
) -> HybridQuoteOutput:
    """Use Anthropic Claude to merge CBR + ML into final quote."""
    client = get_anthropic_client()

    prompt = _format_merger_prompt(request, ml_result, cbr_cases)

    # Use tool calling for structured output
    response = client.messages.create(
        model=settings.anthropic_model,
        max_tokens=4096,
        tools=[{
            "name": "generate_quote",
            "description": "Generate merged roofing quote from CBR and ML predictions",
            "input_schema": HybridQuoteOutput.model_json_schema()
        }],
        tool_choice={"type": "tool", "name": "generate_quote"},
        messages=[{
            "role": "user",
            "content": prompt
        }]
    )

    # Extract structured output from tool call
    tool_use = next(
        (block for block in response.content if block.type == "tool_use"),
        None
    )

    if tool_use is None:
        raise ValueError("LLM did not produce structured output")

    return HybridQuoteOutput.model_validate(tool_use.input)


def _generate_fallback_tiers(base_price: float) -> List[PricingTier]:
    """Generate fallback pricing tiers when LLM unavailable."""
    return [
        PricingTier(
            tier="Basic",
            total_price=round(base_price * 0.85, 2),
            materials_cost=round(base_price * 0.55, 2),
            labor_cost=round(base_price * 0.30, 2),
            description="Essential materials, standard timeline"
        ),
        PricingTier(
            tier="Standard",
            total_price=round(base_price, 2),
            materials_cost=round(base_price * 0.60, 2),
            labor_cost=round(base_price * 0.40, 2),
            description="Full material coverage, standard labor"
        ),
        PricingTier(
            tier="Premium",
            total_price=round(base_price * 1.18, 2),
            materials_cost=round(base_price * 0.65, 2),
            labor_cost=round(base_price * 0.53, 2),
            description="Premium materials, expedited timeline"
        ),
    ]


async def generate_hybrid_quote(
    request: HybridQuoteRequest
) -> HybridQuoteResponse:
    """Main orchestration: parallel CBR+ML, then LLM merger.

    Response time target: <5 seconds
    """
    start_time = time.time()

    # Calculate data completeness upfront
    data_completeness = calculate_data_completeness(
        sqft=request.sqft,
        category=request.category,
        complexity_aggregate=request.complexity_aggregate,
        has_chimney=request.has_chimney,
        has_skylights=request.has_skylights,
        quoted_total=request.quoted_total,
    )

    # Step 1: Run CBR + ML in parallel
    cbr_task = _run_cbr_query(request)
    ml_task = _run_ml_prediction(request)

    results = await asyncio.gather(
        cbr_task,
        ml_task,
        return_exceptions=True
    )

    cbr_result = results[0] if not isinstance(results[0], Exception) else []
    ml_result = results[1] if not isinstance(results[1], Exception) else None

    if isinstance(results[0], Exception):
        logger.warning(f"CBR query failed: {results[0]}")
    if isinstance(results[1], Exception):
        logger.error(f"ML prediction failed: {results[1]}")

    # Step 2: Handle partial failures
    if ml_result is None and not cbr_result:
        raise RuntimeError("Both CBR and ML predictions failed")

    # Extract ML material IDs for confidence scoring
    ml_material_ids = []
    if ml_result:
        ml_material_ids = [
            m["material_id"]
            for m in ml_result.get("materials", {}).get("materials", [])
        ]

    # Step 3: Calculate confidence
    if ml_result is None:
        # CBR-only fallback (rare)
        confidence, needs_review = 0.4, True
        logger.warning("ML failed, using CBR-only with low confidence")
    elif not cbr_result:
        # ML-only fallback
        confidence, needs_review = calculate_confidence_ml_only(
            ml_material_ids=ml_material_ids,
            data_completeness=data_completeness,
        )
        logger.warning("CBR failed, using ML-only")
    else:
        # Normal path: both CBR and ML succeeded
        confidence, needs_review = calculate_confidence(
            cbr_cases=cbr_result,
            ml_material_ids=ml_material_ids,
            data_completeness=data_completeness,
        )

    # Step 4: Merge with LLM (or fallback)
    try:
        merged = await _merge_with_llm(request, ml_result or {}, cbr_result)
    except Exception as e:
        logger.error(f"LLM merger failed: {e}")
        # Fallback: use ML price with generated tiers
        base_price = ml_result.get("price", {}).get("estimate", 0) if ml_result else 0
        merged = HybridQuoteOutput(
            work_items=[],
            materials=[],
            total_labor_hours=0,
            total_materials_cost=ml_result.get("materials", {}).get("total_materials_cost", 0) if ml_result else 0,
            total_price=base_price,
            overall_confidence=confidence,
            reasoning="LLM merger unavailable. Using ML predictions directly.",
            pricing_tiers=_generate_fallback_tiers(base_price),
        )
        needs_review = True  # Always review if LLM failed

    # Step 5: Build response
    processing_time_ms = int((time.time() - start_time) * 1000)

    response = HybridQuoteResponse(
        work_items=merged.work_items,
        materials=merged.materials,
        total_labor_hours=merged.total_labor_hours,
        total_materials_cost=merged.total_materials_cost,
        total_price=merged.total_price,
        overall_confidence=merged.overall_confidence,
        reasoning=merged.reasoning,
        pricing_tiers=merged.pricing_tiers,
        needs_review=needs_review,
        cbr_cases_used=len(cbr_result),
        ml_confidence=ml_result.get("price", {}).get("confidence", "LOW") if ml_result else "LOW",
        processing_time_ms=processing_time_ms,
    )

    logger.info(
        f"Hybrid quote generated in {processing_time_ms}ms "
        f"(CBR={len(cbr_result)} cases, confidence={confidence:.2f})"
    )

    return response
```

Key implementation details from RESEARCH.md:
- Use asyncio.gather() with return_exceptions=True for parallel execution
- Wrap sync functions (CBR query, ML predict) in run_in_executor
- Use Anthropic tool calling with Pydantic schema for structured outputs
- Handle partial failures (CBR-only, ML-only) gracefully
- Generate fallback pricing tiers if LLM fails
- Track processing time for SLA monitoring
- Log confidence breakdown for debugging
</action>
  <verify>
Run import test and validate async function signature:
```bash
cd /Users/aymanbaig/Desktop/Toiture-P1/backend && python -c "
import inspect
from app.services.hybrid_quote import (
    generate_hybrid_quote,
    init_anthropic_client,
    close_anthropic_client,
    _format_merger_prompt,
    _generate_fallback_tiers,
)
from app.schemas.hybrid_quote import HybridQuoteRequest

# Validate generate_hybrid_quote is async (coroutine function)
assert inspect.iscoroutinefunction(generate_hybrid_quote), 'generate_hybrid_quote must be async'
print('Async validation passed: generate_hybrid_quote is a coroutine function')

# Test fallback tiers
tiers = _generate_fallback_tiers(10000)
assert len(tiers) == 3
assert tiers[0].tier == 'Basic'
assert tiers[1].tier == 'Standard'
assert tiers[2].tier == 'Premium'
print(f'Fallback tiers: Basic=\${tiers[0].total_price}, Standard=\${tiers[1].total_price}, Premium=\${tiers[2].total_price}')

# Test prompt formatting (doesn't require API)
request = HybridQuoteRequest(
    sqft=1500,
    category='Bardeaux',
    complexity_aggregate=25,
    access_difficulty=5,
    roof_pitch=4,
    penetrations=3,
    material_removal=4,
    safety_concerns=5,
    timeline_constraints=4,
)
prompt = _format_merger_prompt(request, {'price': {'estimate': 15000}, 'materials': {'materials': [], 'total_materials_cost': 8000}}, [])
assert 'Bardeaux' in prompt
assert '1,500' in prompt
print('Prompt formatting works')

print('Hybrid quote service imports successfully!')
"
```
</verify>
  <done>
- generate_hybrid_quote() is verified as async coroutine using inspect.iscoroutinefunction()
- LLM merger uses Anthropic structured outputs with HybridQuoteOutput schema
- Fallback handling: CBR-only, ML-only, LLM failure all handled gracefully
- Confidence scoring integrated from 13-02
- Processing time tracked for <5s SLA monitoring
</done>
</task>

</tasks>

<verification>
1. Service imports without errors
2. generate_hybrid_quote is validated as async coroutine function
3. Anthropic client init/close lifecycle methods work
4. Prompt formatting produces reasonable LLM input
5. Fallback tiers generated correctly
6. Async structure allows parallel CBR+ML execution
</verification>

<success_criteria>
- backend/app/services/hybrid_quote.py exists with 200+ lines
- config.py includes anthropic_api_key setting
- generate_hybrid_quote() is async and uses asyncio.gather()
- LLM merger uses Anthropic tool calling with Pydantic schema
- Partial failures (CBR/ML/LLM) handled gracefully
- Processing time tracked in response
</success_criteria>

<output>
After completion, create `.planning/phases/13-hybrid-quote-generation/13-03-SUMMARY.md`
</output>
