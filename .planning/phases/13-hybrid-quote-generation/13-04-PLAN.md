---
phase: 13-hybrid-quote-generation
plan: 04
type: execute
wave: 4
depends_on: ["13-01", "13-02", "13-03"]
files_modified:
  - backend/app/routers/estimate.py
  - backend/app/main.py
  - backend/.env.example
autonomous: true

must_haves:
  truths:
    - "POST /estimate/hybrid endpoint accepts HybridQuoteRequest and returns HybridQuoteResponse"
    - "Service call detection routes labor-only jobs to separate flow"
    - "Anthropic client initializes on startup via lifespan"
    - "Response time is tracked and logged for <5s SLA"
  artifacts:
    - path: "backend/app/routers/estimate.py"
      provides: "POST /estimate/hybrid endpoint"
      contains: "/estimate/hybrid"
    - path: "backend/app/main.py"
      provides: "Anthropic client lifecycle management"
      contains: "init_anthropic_client"
    - path: "backend/.env.example"
      provides: "ANTHROPIC_API_KEY documentation"
      contains: "ANTHROPIC_API_KEY"
  key_links:
    - from: "backend/app/routers/estimate.py"
      to: "backend/app/services/hybrid_quote.py"
      via: "generate_hybrid_quote import"
      pattern: "from app.services.hybrid_quote import"
    - from: "backend/app/main.py"
      to: "backend/app/services/hybrid_quote.py"
      via: "lifespan init/close"
      pattern: "init_anthropic_client|close_anthropic_client"
---

<objective>
Create the POST /estimate/hybrid API endpoint, integrate Anthropic client lifecycle into app startup, and update environment documentation.

Purpose: This plan wires the hybrid quote orchestrator into the FastAPI application, making it available as an HTTP endpoint. The endpoint includes service call detection to route labor-only jobs appropriately.

Output: Functional /estimate/hybrid endpoint + updated app lifecycle + env documentation
</objective>

<execution_context>
@/Users/aymanbaig/.claude/get-shit-done/workflows/execute-plan.md
@/Users/aymanbaig/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/13-hybrid-quote-generation/13-CONTEXT.md
@.planning/phases/13-hybrid-quote-generation/13-RESEARCH.md
@.planning/phases/13-hybrid-quote-generation/13-01-SUMMARY.md
@.planning/phases/13-hybrid-quote-generation/13-02-SUMMARY.md
@.planning/phases/13-hybrid-quote-generation/13-03-SUMMARY.md

# Files to modify
@backend/app/routers/estimate.py
@backend/app/main.py
@backend/.env.example
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Anthropic client lifecycle to main.py</name>
  <files>backend/app/main.py</files>
  <action>
Update the FastAPI lifespan context manager to initialize and close the Anthropic client:

1. Add import at top of file:
```python
from app.services.hybrid_quote import init_anthropic_client, close_anthropic_client
```

2. In the lifespan function, add after other service initializations:
```python
# Initialize Anthropic client for hybrid quotes
init_anthropic_client()
```

3. In the shutdown section (after yield), add:
```python
# Close Anthropic client
close_anthropic_client()
```

This follows the same pattern as init_pinecone(), init_llm_client(), etc.
</action>
  <verify>
Check that main.py imports and calls the functions:
```bash
cd /Users/aymanbaig/Desktop/Toiture-P1/backend && grep -n "anthropic" app/main.py
```
Should show import and both init/close calls.
</verify>
  <done>
main.py lifespan initializes and closes Anthropic client on app startup/shutdown
</done>
</task>

<task type="auto">
  <name>Task 2: Add POST /estimate/hybrid endpoint</name>
  <files>backend/app/routers/estimate.py</files>
  <action>
Add the hybrid quote endpoint to the estimate router:

1. Add imports at top of file (these are shared with rest of file):
```python
from app.schemas.hybrid_quote import HybridQuoteRequest, HybridQuoteResponse, PricingTier
from app.services.hybrid_quote import generate_hybrid_quote
```

2. Add the endpoint after existing /estimate/full endpoint:
```python
@router.post("/estimate/hybrid", response_model=HybridQuoteResponse)
async def create_hybrid_estimate(request: HybridQuoteRequest):
    """Generate full hybrid quote using CBR + ML + LLM merger.

    This endpoint:
    1. Runs CBR (similar case retrieval) and ML (material prediction) in parallel
    2. Merges results using Claude LLM with structured outputs
    3. Generates three-tier pricing (Basic/Standard/Premium)
    4. Returns confidence score and review flag

    Service calls (labor-only jobs) are detected and handled separately.

    Response time target: <5 seconds
    """
    # Service call detection: skip materials pipeline for labor-only jobs
    is_service_call = (
        request.material_lines == 0 or
        request.sqft < 100
    )

    if is_service_call:
        logger.info(f"Service call detected (sqft={request.sqft}, material_lines={request.material_lines})")
        # For service calls, use simple price prediction only
        try:
            price_result = predict(
                sqft=request.sqft,
                category=request.category,
                material_lines=request.material_lines,
                labor_lines=request.labor_lines,
                has_subs=1 if request.has_subs else 0,
                complexity=request.complexity_aggregate,
            )

            # Service call response: labor only, no materials
            # Note: PricingTier imported at top of file with other hybrid_quote imports
            return HybridQuoteResponse(
                work_items=[],
                materials=[],
                total_labor_hours=request.labor_lines * 2.0,  # Rough estimate
                total_materials_cost=0,
                total_price=price_result["estimate"],
                overall_confidence=0.6,  # Service calls are straightforward
                reasoning="Service call detected. Labor-only estimate based on ML prediction.",
                pricing_tiers=[
                    PricingTier(
                        tier="Basic",
                        total_price=round(price_result["estimate"] * 0.9, 2),
                        materials_cost=0,
                        labor_cost=round(price_result["estimate"] * 0.9, 2),
                        description="Standard service call"
                    ),
                    PricingTier(
                        tier="Standard",
                        total_price=round(price_result["estimate"], 2),
                        materials_cost=0,
                        labor_cost=round(price_result["estimate"], 2),
                        description="Service call with inspection"
                    ),
                    PricingTier(
                        tier="Premium",
                        total_price=round(price_result["estimate"] * 1.2, 2),
                        materials_cost=0,
                        labor_cost=round(price_result["estimate"] * 1.2, 2),
                        description="Emergency/rush service call"
                    ),
                ],
                needs_review=False,  # Service calls are low complexity
                cbr_cases_used=0,
                ml_confidence=price_result["confidence"],
                processing_time_ms=50,  # Fast path
            )
        except Exception as e:
            logger.error(f"Service call estimate error: {e}")
            raise HTTPException(status_code=500, detail=str(e))

    # Normal path: full hybrid quote generation
    try:
        response = await generate_hybrid_quote(request)
        return response
    except RuntimeError as e:
        # Both CBR and ML failed
        logger.error(f"Hybrid quote failed: {e}")
        raise HTTPException(status_code=503, detail=str(e))
    except Exception as e:
        logger.error(f"Hybrid quote error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

Key implementation details:
- Service call detection: material_lines == 0 OR sqft < 100
- Service calls skip the full hybrid pipeline (fast path)
- Normal jobs use async generate_hybrid_quote()
- 503 returned when both CBR and ML fail
- 500 returned for other errors
- All schema imports (HybridQuoteRequest, HybridQuoteResponse, PricingTier) are at top of file - no inline imports
</action>
  <verify>
Check endpoint is defined:
```bash
cd /Users/aymanbaig/Desktop/Toiture-P1/backend && python -c "
from app.main import app
routes = [r.path for r in app.routes if hasattr(r, 'path')]
print('Routes:', routes)
assert '/estimate/hybrid' in routes, 'Missing /estimate/hybrid route'
print('/estimate/hybrid endpoint registered!')
"
```
</verify>
  <done>
- POST /estimate/hybrid endpoint accepts HybridQuoteRequest
- Service call detection routes labor-only jobs to fast path
- Normal jobs use full hybrid quote generation
- Appropriate error codes: 503 for total failure, 500 for other errors
- All imports consolidated at top of file (no redundant inline imports)
</done>
</task>

<task type="auto">
  <name>Task 3: Update .env.example with Anthropic key</name>
  <files>backend/.env.example</files>
  <action>
Add Anthropic API key documentation to .env.example:

```
# Anthropic API (hybrid quote LLM merger)
# Get API key from: https://console.anthropic.com/settings/keys
ANTHROPIC_API_KEY=
# Model for structured outputs (default: claude-sonnet-4-5-20250514)
ANTHROPIC_MODEL=claude-sonnet-4-5-20250514
```

Add this after the OpenRouter settings section.

This documents the new environment variable requirement for hybrid quote generation.
</action>
  <verify>
```bash
cd /Users/aymanbaig/Desktop/Toiture-P1/backend && grep -n "ANTHROPIC" .env.example
```
Should show ANTHROPIC_API_KEY and ANTHROPIC_MODEL entries.
</verify>
  <done>
.env.example documents ANTHROPIC_API_KEY requirement with setup instructions
</done>
</task>

</tasks>

<verification>
1. main.py initializes Anthropic client in lifespan
2. /estimate/hybrid endpoint is registered in app routes
3. Service call detection works (material_lines=0 or sqft<100)
4. .env.example documents ANTHROPIC_API_KEY
5. App starts without errors when ANTHROPIC_API_KEY is not set (graceful degradation)
</verification>

<success_criteria>
- POST /estimate/hybrid endpoint exists and accepts HybridQuoteRequest
- Service calls (sqft < 100 or material_lines = 0) use fast path
- Anthropic client lifecycle managed in main.py lifespan
- .env.example includes ANTHROPIC_API_KEY documentation
- App starts successfully (even without API key - with warning)
</success_criteria>

<output>
After completion, create `.planning/phases/13-hybrid-quote-generation/13-04-SUMMARY.md`
</output>
