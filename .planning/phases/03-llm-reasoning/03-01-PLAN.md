---
phase: 03-llm-reasoning
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/services/llm_reasoning.py
  - backend/app/config.py
  - backend/requirements.txt
autonomous: true
user_setup:
  - service: openrouter
    why: "LLM API access for reasoning generation"
    env_vars:
      - name: OPENROUTER_API_KEY
        source: "OpenRouter Dashboard -> Keys (https://openrouter.ai/keys)"

must_haves:
  truths:
    - "OpenRouter client initializes at startup without error"
    - "generate_reasoning() produces 2-3 sentence explanation"
    - "LLM errors are caught and don't crash the service"
  artifacts:
    - path: "backend/app/services/llm_reasoning.py"
      provides: "LLM client initialization and reasoning generation"
      exports: ["init_llm_client", "close_llm_client", "generate_reasoning"]
    - path: "backend/app/config.py"
      provides: "OpenRouter configuration settings"
      contains: "openrouter_api_key"
    - path: "backend/requirements.txt"
      provides: "LLM dependencies"
      contains: "openai"
  key_links:
    - from: "backend/app/services/llm_reasoning.py"
      to: "backend/app/config.py"
      via: "settings import"
      pattern: "from.*config import settings"
---

<objective>
Create LLM reasoning service with OpenRouter client initialization and reasoning generation function.

Purpose: Enable the estimate endpoint to generate human-readable explanations that reference similar cases and explain confidence levels.

Output: Working llm_reasoning.py service module, updated config.py with OpenRouter settings, updated requirements.txt with openai and tenacity packages.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-llm-reasoning/03-RESEARCH.md
@backend/app/config.py
@backend/app/services/predictor.py
@backend/requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add OpenRouter dependencies and config</name>
  <files>backend/requirements.txt, backend/app/config.py</files>
  <action>
1. Add to requirements.txt:
   - openai>=1.0.0 (OpenRouter is OpenAI-compatible)
   - tenacity>=8.2.0 (retry with exponential backoff)

2. Update backend/app/config.py Settings class to add:
   - openrouter_api_key: str = "" (required for LLM)
   - openrouter_model: str = "openai/gpt-4o-mini" (cost-effective default)
   - openrouter_base_url: str = "https://openrouter.ai/api/v1"
   - app_url: str = "https://toiturelv-cortex.railway.app" (for HTTP-Referer header)

Use same pattern as existing pinecone_api_key (empty string default, loaded from env).
  </action>
  <verify>
Run: `pip install -r backend/requirements.txt`
Check: No errors, openai and tenacity packages installed
  </verify>
  <done>
requirements.txt includes openai>=1.0.0 and tenacity>=8.2.0. config.py Settings class has openrouter_api_key, openrouter_model, openrouter_base_url, and app_url fields.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create LLM reasoning service module</name>
  <files>backend/app/services/llm_reasoning.py</files>
  <action>
Create backend/app/services/llm_reasoning.py with:

1. Module-level client storage pattern (same as predictor.py and pinecone_cbr.py):
   - _client: Optional[OpenAI] = None

2. init_llm_client() function:
   - Creates OpenAI client with OpenRouter base_url
   - Sets 30s timeout
   - Includes HTTP-Referer and X-Title headers
   - Logs initialization
   - Called from lifespan

3. close_llm_client() function:
   - Sets _client to None
   - Called from lifespan shutdown

4. get_client() -> OpenAI function:
   - Returns _client or raises RuntimeError if not initialized

5. format_similar_cases(cases: List[Dict[str, Any]]) -> str function:
   - Takes similar cases list
   - Returns formatted string for prompt (max 5 cases)
   - Format: "1. Category (year): $total, sqft sqft, $per_sqft/sqft, X% similar"
   - Returns "No similar historical cases found." if empty

6. generate_reasoning() function with @retry decorator:
   - Parameters: estimate, confidence, sqft, category, similar_cases, model (optional)
   - Retry: 3 attempts, exponential backoff (2-10s), retry on AuthenticationError, APITimeoutError, APIConnectionError, RateLimitError
   - Builds prompt with job details, confidence context, and formatted similar cases
   - Uses temperature=0.3, max_tokens=150
   - Returns reasoning string (2-3 sentences)
   - System prompt: "You are a roofing estimation assistant. Be concise, professional, and reference specific data from similar jobs."

Follow exact patterns from 03-RESEARCH.md code examples.
  </action>
  <verify>
Run: `python -c "from backend.app.services.llm_reasoning import init_llm_client, close_llm_client, generate_reasoning; print('Imports OK')"`
Check: No import errors
  </verify>
  <done>
llm_reasoning.py exists with init_llm_client, close_llm_client, get_client, format_similar_cases, and generate_reasoning functions. Module follows same pattern as other services (module-level client storage, init/close lifecycle).
  </done>
</task>

<task type="auto">
  <name>Task 3: Verify service module in isolation</name>
  <files>backend/app/services/llm_reasoning.py</files>
  <action>
Create a quick verification that the module structure is correct:

1. Run Python to verify imports work:
   python -c "
   from backend.app.services.llm_reasoning import (
       init_llm_client,
       close_llm_client,
       generate_reasoning,
       format_similar_cases,
   )
   print('All exports available')
   "

2. Test format_similar_cases with sample data:
   python -c "
   from backend.app.services.llm_reasoning import format_similar_cases
   cases = [
       {'case_id': '1', 'similarity': 0.95, 'category': 'Bardeaux', 'sqft': 2500, 'total': 15000, 'per_sqft': 6.0, 'year': 2023},
       {'case_id': '2', 'similarity': 0.88, 'category': 'Bardeaux', 'sqft': 2200, 'total': 13000, 'per_sqft': 5.9, 'year': 2024},
   ]
   print(format_similar_cases(cases))
   "

3. Verify error handling for uninitialized client:
   python -c "
   from backend.app.services.llm_reasoning import get_client
   try:
       get_client()
   except RuntimeError as e:
       print(f'Correct error: {e}')
   "
  </action>
  <verify>
All three verification commands complete without errors.
format_similar_cases outputs formatted case text.
get_client raises RuntimeError when not initialized.
  </verify>
  <done>
LLM reasoning service module verified: exports work, format_similar_cases formats data correctly, get_client enforces initialization.
  </done>
</task>

</tasks>

<verification>
1. `pip install -r backend/requirements.txt` succeeds
2. `python -c "from backend.app.services.llm_reasoning import init_llm_client, generate_reasoning"` succeeds
3. `python -c "from backend.app.config import settings; print(settings.openrouter_model)"` prints model name
4. format_similar_cases produces readable output
5. get_client raises RuntimeError before initialization
</verification>

<success_criteria>
- requirements.txt has openai>=1.0.0 and tenacity>=8.2.0
- config.py has openrouter_api_key, openrouter_model, openrouter_base_url, app_url settings
- llm_reasoning.py has init_llm_client, close_llm_client, generate_reasoning, format_similar_cases
- All imports work without errors
- Module follows established service patterns (module-level client, init/close lifecycle)
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-reasoning/03-01-SUMMARY.md`
</output>
