# 12-01: Data Quality Flags + 2022 Filtering

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Add data quality flags to filter out corrupted 2022 labor data from ML training and queries.

**Architecture:** Add `data_quality_flags` JSONB column to estimates table. Create backend filtering to exclude flagged records. Update query endpoints to support flag-based filtering.

**Tech Stack:** Supabase (PostgreSQL), FastAPI, Pydantic, TypeScript

---

## Context

Laurent identified 1,512 quotes from 2022 with corrupted labor hours. The "flat roof day" was coded as 1 hour at $7,200 instead of 56 hours (7 workers Ã— 8 hours). These records should be flagged and excluded from ML training.

## Plan

### Task 1: Add Database Column via Supabase Migration

**Files:**
- Create: `backend/migrations/add_data_quality_flags.sql`

**Step 1: Create migration SQL file**

```sql
-- Migration: Add data quality flags to estimates table
-- Purpose: Flag corrupted 2022 labor data for exclusion from ML training

-- Add data_quality_flags JSONB column
ALTER TABLE estimates
ADD COLUMN IF NOT EXISTS data_quality_flags JSONB DEFAULT '{}';

-- Add index for common flag queries
CREATE INDEX IF NOT EXISTS idx_estimates_quality_flags
ON estimates USING gin(data_quality_flags);

-- Add labor_data_reliable convenience column for fast filtering
ALTER TABLE estimates
ADD COLUMN IF NOT EXISTS labor_data_reliable BOOLEAN DEFAULT true;

-- Create index for labor_data_reliable filtering
CREATE INDEX IF NOT EXISTS idx_estimates_labor_reliable
ON estimates (labor_data_reliable) WHERE labor_data_reliable = false;

-- Flag 2022 records as having unreliable labor data
-- (Records from 2022 where the "flat roof day" bug affected labor hours)
UPDATE estimates
SET
  labor_data_reliable = false,
  data_quality_flags = jsonb_set(
    COALESCE(data_quality_flags, '{}'),
    '{labor_hours_corrupted}',
    'true'
  )
WHERE
  created_at >= '2022-01-01'::timestamp
  AND created_at < '2023-01-01'::timestamp;

-- Add comment explaining the flag
COMMENT ON COLUMN estimates.data_quality_flags IS 'JSONB flags for data quality issues. Keys: labor_hours_corrupted, sqft_missing, manually_reviewed';
COMMENT ON COLUMN estimates.labor_data_reliable IS 'False for 2022 records with corrupted labor hours (flat roof day bug)';
```

**Step 2: Apply migration via Supabase**

Run in Supabase SQL Editor or via CLI:
```bash
# If using Supabase CLI
supabase db push
```

**Step 3: Verify migration**

```sql
-- Verify columns exist
SELECT column_name, data_type
FROM information_schema.columns
WHERE table_name = 'estimates'
AND column_name IN ('data_quality_flags', 'labor_data_reliable');

-- Count flagged records
SELECT COUNT(*) as flagged_count
FROM estimates
WHERE labor_data_reliable = false;
-- Expected: ~1,512 records
```

---

### Task 2: Update Backend Schemas

**Files:**
- Modify: `backend/app/schemas/quotes.py`
- Modify: `backend/app/schemas/feedback.py`

**Step 1: Update quotes.py with data quality fields**

Add to `backend/app/schemas/quotes.py`:

```python
from typing import Optional, Dict, Any

class DataQualityFlags(BaseModel):
    """Data quality flags for ML training exclusion"""
    labor_hours_corrupted: bool = False
    sqft_missing: bool = False
    manually_reviewed: bool = False

class QuoteItem(BaseModel):
    """Individual quote in listing"""
    id: str
    client_name: Optional[str] = None
    category: Optional[str] = None
    city: Optional[str] = None
    sqft: Optional[float] = None
    total_price: Optional[float] = None
    margin_percent: Optional[float] = None
    created_at: Optional[str] = None
    # NEW: Data quality fields
    labor_data_reliable: bool = True
    data_quality_flags: Optional[Dict[str, Any]] = None

class QuoteFilters(BaseModel):
    """Query filters for quote listing"""
    category: Optional[str] = None
    city: Optional[str] = None
    min_sqft: Optional[float] = None
    max_sqft: Optional[float] = None
    min_price: Optional[float] = None
    max_price: Optional[float] = None
    start_date: Optional[str] = None
    end_date: Optional[str] = None
    # NEW: Data quality filters
    labor_data_reliable: Optional[bool] = None  # None = all, True = only reliable, False = only unreliable
    exclude_flagged: bool = False  # If True, exclude all flagged records
```

**Step 2: Update feedback.py**

Add to `backend/app/schemas/feedback.py` EstimateListItem:

```python
class EstimateListItem(BaseModel):
    """Estimate item in review queue"""
    id: str
    sqft: float
    category: str
    # ... existing fields ...
    # NEW: Data quality indicator
    labor_data_reliable: bool = True
```

---

### Task 3: Update Backend Router for Filtering

**Files:**
- Modify: `backend/app/routers/quotes.py`

**Step 1: Add filtering logic**

Update the GET /quotes endpoint in `backend/app/routers/quotes.py`:

```python
@router.get("/", response_model=PaginatedQuotes)
async def list_quotes(
    page: int = Query(1, ge=1),
    per_page: int = Query(20, ge=1, le=100),
    category: Optional[str] = None,
    city: Optional[str] = None,
    min_sqft: Optional[float] = Query(None, ge=0),
    max_sqft: Optional[float] = Query(None, ge=0),
    min_price: Optional[float] = Query(None, ge=0),
    max_price: Optional[float] = Query(None, ge=0),
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    # NEW: Data quality filters
    labor_data_reliable: Optional[bool] = None,
    exclude_flagged: bool = False,
):
    """
    List quotes with pagination and filtering.

    New filters:
    - labor_data_reliable: Filter by labor data reliability (None=all, True=reliable only)
    - exclude_flagged: Exclude all records with any data quality flags
    """
    try:
        supabase = get_supabase_client()

        # Build query with new columns
        query = supabase.table("estimates").select(
            "id, client_name, category, city, sqft, ai_estimate, margin_percent, created_at, "
            "labor_data_reliable, data_quality_flags",
            count="exact"
        )

        # Existing filters...
        if category:
            query = query.eq("category", category)
        if city:
            query = query.ilike("city", f"%{city}%")
        if min_sqft is not None:
            query = query.gte("sqft", min_sqft)
        if max_sqft is not None:
            query = query.lte("sqft", max_sqft)
        if min_price is not None:
            query = query.gte("ai_estimate", min_price)
        if max_price is not None:
            query = query.lte("ai_estimate", max_price)
        if start_date:
            query = query.gte("created_at", start_date)
        if end_date:
            query = query.lte("created_at", end_date)

        # NEW: Data quality filters
        if labor_data_reliable is not None:
            query = query.eq("labor_data_reliable", labor_data_reliable)

        if exclude_flagged:
            query = query.eq("labor_data_reliable", True)

        # Pagination and ordering
        offset = (page - 1) * per_page
        query = query.order("created_at", desc=True).range(offset, offset + per_page - 1)

        result = query.execute()

        # Map results with data quality fields
        items = [
            QuoteItem(
                id=row["id"],
                client_name=row.get("client_name"),
                category=row.get("category"),
                city=row.get("city"),
                sqft=row.get("sqft"),
                total_price=row.get("ai_estimate"),
                margin_percent=row.get("margin_percent"),
                created_at=row.get("created_at"),
                labor_data_reliable=row.get("labor_data_reliable", True),
                data_quality_flags=row.get("data_quality_flags"),
            )
            for row in result.data
        ]

        total = result.count or 0
        total_pages = (total + per_page - 1) // per_page

        return PaginatedQuotes(
            items=items,
            total=total,
            page=page,
            per_page=per_page,
            total_pages=total_pages,
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

### Task 4: Update Frontend Types

**Files:**
- Modify: `frontend/src/types/quote.ts`

**Step 1: Add data quality types**

```typescript
export interface DataQualityFlags {
  labor_hours_corrupted?: boolean;
  sqft_missing?: boolean;
  manually_reviewed?: boolean;
}

export interface Quote {
  id: string;
  client_name?: string;
  category?: string;
  city?: string;
  sqft?: number;
  total_price?: number;
  margin_percent?: number;
  created_at?: string;
  // NEW: Data quality fields
  labor_data_reliable: boolean;
  data_quality_flags?: DataQualityFlags;
}

export interface QuoteFilters {
  category?: string;
  city?: string;
  min_sqft?: number;
  max_sqft?: number;
  min_price?: number;
  max_price?: number;
  start_date?: string;
  end_date?: string;
  // NEW: Data quality filters
  labor_data_reliable?: boolean;
  exclude_flagged?: boolean;
}
```

---

### Task 5: Add Visual Indicator in Quote Table

**Files:**
- Modify: `frontend/src/components/historique/quote-table.tsx`

**Step 1: Add warning badge for flagged records**

Add a visual indicator when `labor_data_reliable` is false:

```tsx
import { AlertTriangle } from "lucide-react";
import { Badge } from "@/components/ui/badge";

// In the table row rendering:
{!quote.labor_data_reliable && (
  <Badge variant="destructive" className="ml-2">
    <AlertTriangle className="h-3 w-3 mr-1" />
    2022 Data
  </Badge>
)}
```

---

### Task 6: Verify and Test

**Step 1: Build verification**

```bash
cd backend && python -m pytest tests/ -v
cd frontend && npm run build
```

**Step 2: API test**

```bash
# Test filtering for reliable data only
curl "http://localhost:8000/quotes?labor_data_reliable=true&per_page=5"

# Test showing flagged records
curl "http://localhost:8000/quotes?labor_data_reliable=false&per_page=5"

# Test exclude_flagged
curl "http://localhost:8000/quotes?exclude_flagged=true&per_page=5"
```

**Step 3: Commit**

```bash
git add -A
git commit -m "feat(data-quality): add flags to filter corrupted 2022 labor data

- Add data_quality_flags JSONB column to estimates table
- Add labor_data_reliable boolean for fast filtering
- Flag all 2022 records as having unreliable labor data
- Update quotes API with filtering support
- Add visual indicator in quote table for flagged records

Closes: Laurent feedback issue #2"
```

---

## Success Criteria

- [ ] `data_quality_flags` and `labor_data_reliable` columns exist in estimates table
- [ ] ~1,512 records from 2022 are flagged with `labor_data_reliable = false`
- [ ] GET /quotes supports `labor_data_reliable` and `exclude_flagged` filters
- [ ] Quote table shows visual warning for flagged records
- [ ] Build passes for both frontend and backend
