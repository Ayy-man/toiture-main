---
phase: 27-ai-chat-interface
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/schemas/chat.py
  - backend/app/services/chat_session.py
  - backend/app/services/chat_extraction.py
  - backend/app/routers/chat.py
  - backend/app/main.py
autonomous: true

must_haves:
  truths:
    - "POST /chat/message accepts session_id + message and returns structured reply with extracted fields"
    - "LLM extracts roofing job parameters from natural language input using GPT-4o-mini"
    - "Conversation state persists across multiple messages within a session"
    - "When enough fields are extracted, quote is auto-generated via existing hybrid_quote service"
    - "Response includes context-aware suggestion pills for common follow-up values"
  artifacts:
    - path: "backend/app/schemas/chat.py"
      provides: "ChatMessageRequest and ChatMessageResponse Pydantic models"
      contains: "class ChatMessageRequest"
    - path: "backend/app/services/chat_session.py"
      provides: "In-memory session store with conversation history and extracted fields"
      contains: "get_session"
    - path: "backend/app/services/chat_extraction.py"
      provides: "LLM-powered field extraction from natural language using OpenRouter"
      contains: "extract_fields"
    - path: "backend/app/routers/chat.py"
      provides: "POST /chat/message endpoint"
      contains: "router"
  key_links:
    - from: "backend/app/routers/chat.py"
      to: "backend/app/services/chat_extraction.py"
      via: "extract_fields() call"
      pattern: "extract_fields"
    - from: "backend/app/services/chat_extraction.py"
      to: "backend/app/services/llm_reasoning.py"
      via: "get_client() for OpenRouter access"
      pattern: "get_client"
    - from: "backend/app/routers/chat.py"
      to: "backend/app/services/hybrid_quote.py"
      via: "generate_hybrid_quote() when fields are complete"
      pattern: "generate_hybrid_quote"
    - from: "backend/app/main.py"
      to: "backend/app/routers/chat.py"
      via: "app.include_router(chat.router)"
      pattern: "include_router.*chat"
---

<objective>
Build the backend conversational endpoint for the AI chat interface. This creates the POST /chat/message endpoint that accepts natural language descriptions of roofing jobs, extracts structured fields using GPT-4o-mini via the existing OpenRouter client, manages conversation state in-memory, and auto-triggers hybrid quote generation when enough fields are collected.

Purpose: Steven needs to describe a roofing job in natural language and get a structured quote. This plan creates the backend intelligence that powers that conversation.

Output: Working POST /chat/message endpoint that maps natural language to HybridQuoteRequest fields, maintains session context, generates quotes when ready, and returns suggestion pills.
</objective>

<execution_context>
@/Users/aymanbaig/.claude/get-shit-done/workflows/execute-plan.md
@/Users/aymanbaig/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/27-ai-chat-interface/DESIGN.md
@.planning/phases/27-ai-chat-interface/27-RESEARCH.md
@backend/app/services/llm_reasoning.py
@backend/app/services/hybrid_quote.py
@backend/app/schemas/hybrid_quote.py
@backend/app/routers/estimate.py
@backend/app/main.py
@backend/app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Chat schemas, session store, and field extraction service</name>
  <files>
    backend/app/schemas/chat.py
    backend/app/services/chat_session.py
    backend/app/services/chat_extraction.py
  </files>
  <action>
Create three files:

**1. backend/app/schemas/chat.py** — Pydantic models for the chat endpoint:

- `ChatMessageRequest(BaseModel)`:
  - `session_id: str` — UUID for conversation tracking
  - `message: str` — User's natural language input (min_length=1, max_length=2000)
  - `language: Optional[str] = "fr"` — Language code ("fr" or "en") for bilingual responses

- `ChatMessageResponse(BaseModel)`:
  - `reply: str` — Assistant's natural language response
  - `extracted_fields: Dict[str, Any]` — Current cumulative extracted fields (merged from all messages)
  - `suggestions: List[str]` — Quick-reply suggestion pills (3-5 context-aware options)
  - `quote: Optional[Any] = None` — HybridQuoteResponse when quote is generated (use Any to avoid circular import, will be HybridQuoteResponse at runtime)
  - `needs_clarification: bool` — True if critical fields are still missing
  - `session_state: str` — Current conversation state: "greeting" | "extracting" | "clarifying" | "ready" | "generated"

- `ChatSession(BaseModel)`:
  - `session_id: str`
  - `messages: List[Dict[str, str]]` — Conversation history as [{"role": "user"|"assistant", "content": "..."}]
  - `extracted_fields: Dict[str, Any]` — Accumulated extracted fields
  - `state: str` — Current state in conversation flow
  - `created_at: str` — ISO timestamp
  - `language: str` — Session language

**2. backend/app/services/chat_session.py** — In-memory session store:

- Module-level dict `_sessions: Dict[str, Dict[str, Any]] = {}` (same pattern as predictor.py module-level storage)
- `get_session(session_id: str) -> Dict[str, Any]`: Returns existing session or creates new one with defaults: `{"messages": [], "extracted_fields": {}, "state": "greeting", "created_at": datetime.utcnow().isoformat(), "language": "fr"}`
- `update_session(session_id: str, messages: List, extracted_fields: Dict, state: str) -> None`: Updates session in place
- `clear_session(session_id: str) -> None`: Removes session from dict
- `get_session_count() -> int`: Returns count of active sessions (for health check)
- Add TTL cleanup: sessions older than 24 hours are evicted on each `get_session` call (iterate dict, remove stale entries). Keep it simple — no background thread.

**3. backend/app/services/chat_extraction.py** — LLM-powered field extraction:

- `EXTRACTION_SYSTEM_PROMPT` — System prompt in both FR and EN variants. The prompt tells GPT-4o-mini to:
  - Act as a Quebec roofing estimator assistant for Toitures LV
  - Extract structured fields from natural language into a JSON schema matching HybridQuoteRequest fields
  - Map French roofing terms: "bardeaux" -> category "Bardeaux", "membrane" -> "Membrane/Elastomere", "TPO" -> "TPO", "toit plat" -> flat roof factors, "pente raide" -> steep pitch
  - Return a JSON object with two keys: `extracted` (dict of fields) and `reply` (natural language response)
  - If critical fields missing (sqft, category), ask for them in the reply
  - Suggest complexity tier (1-6) based on described conditions
  - Include the full HybridQuoteRequest JSON schema in the prompt for reference

- `REQUIRED_FIELDS = ["sqft", "category"]` — Minimum fields needed to generate a quote (sqft not required for Service Call)

- `SUGGESTION_MAP` — Dict mapping conversation states to suggestion lists:
  - "greeting": ["Bardeaux", "Membrane", "TPO", "Appel de service", "Toit plat"]
  - "need_sqft": ["500 pi2", "1000 pi2", "1500 pi2", "2000 pi2", "2500 pi2"]
  - "need_category": ["Bardeaux", "Membrane/Elastomere", "TPO", "Service Call"]
  - "need_complexity": ["Simple (Tier 1-2)", "Moyen (Tier 3-4)", "Complexe (Tier 5-6)"]
  - "ready": ["Generer le devis", "Ajouter des details", "Changer la superficie"]

- `async def extract_fields(message: str, conversation_history: List[Dict], current_fields: Dict, language: str = "fr") -> Dict`:
  - Uses `llm_reasoning.get_client()` to access OpenRouter
  - Sends conversation history + new message with system prompt
  - Model: `openai/gpt-4o-mini` (from settings.openrouter_model)
  - Parses JSON response from LLM (with regex fallback like hybrid_quote.py pattern)
  - Returns dict with keys: `extracted` (new fields to merge), `reply` (assistant text), `suggestions` (list of pills)
  - Uses temperature=0.2 for reliable extraction
  - max_tokens=500 for response
  - Includes retry logic via tenacity (same pattern as llm_reasoning.py)

- `def check_readiness(extracted_fields: Dict) -> Tuple[bool, List[str]]`:
  - Checks if minimum required fields are present
  - Returns (is_ready: bool, missing_fields: List[str])
  - For Service Call category: sqft not required
  - For other categories: sqft and category required

- `def get_suggestions(state: str, extracted_fields: Dict, language: str) -> List[str]`:
  - Returns context-aware suggestion pills based on current state and what's missing
  - Uses SUGGESTION_MAP as base, with English variants when language="en"
  </action>
  <verify>
    Run: `cd backend && python -c "from app.schemas.chat import ChatMessageRequest, ChatMessageResponse; from app.services.chat_session import get_session, update_session; from app.services.chat_extraction import extract_fields, check_readiness; print('All imports OK')"` — Should print "All imports OK" with no errors.
  </verify>
  <done>
    Three files exist with all classes/functions importable. ChatMessageRequest validates input, ChatMessageResponse shapes output, session store manages conversation state in-memory with TTL cleanup, and extraction service has complete system prompt with field mapping logic.
  </done>
</task>

<task type="auto">
  <name>Task 2: Chat router endpoint and main.py registration</name>
  <files>
    backend/app/routers/chat.py
    backend/app/main.py
  </files>
  <action>
**1. backend/app/routers/chat.py** — Chat API router:

Create a new router file following the exact pattern of `backend/app/routers/submissions.py`:

- `router = APIRouter(prefix="/chat", tags=["chat"])`

- `POST /chat/message` endpoint:
  ```python
  @router.post("/message", response_model=ChatMessageResponse)
  async def send_message(request: ChatMessageRequest):
  ```
  Logic:
  1. Get or create session via `chat_session.get_session(request.session_id)`
  2. If state is "greeting" and session has no messages, set reply to a greeting message (bilingual based on request.language):
     - FR: "Bonjour! Decrivez le projet de toiture et je vais preparer le devis. Par exemple: '1200 pi2, bardeaux, pente raide, acces difficile'"
     - EN: "Hi! Describe the roofing job and I'll build the quote. For example: '1200 sqft, shingles, steep pitch, difficult access'"
     - Append the greeting as an assistant message to conversation history
     - Return with greeting suggestions and state="extracting"
     - NOTE: Only greet if the user's message is a greeting-like input (empty, "bonjour", "hi", "hello", "salut"). If user sends real content in first message, skip to extraction.
  3. Append user message to conversation history
  4. Call `extract_fields()` with message, conversation history, current extracted fields, language
  5. Merge newly extracted fields into session's accumulated extracted_fields (new values overwrite old, don't lose existing values)
  6. Call `check_readiness()` on merged fields
  7. If ready AND user explicitly asked to generate (message contains "generer", "generate", "devis", "quote", "oui", "yes", "go"):
     - Build a `HybridQuoteRequest` from extracted_fields (map field names, fill defaults for missing optional fields)
     - Call `generate_hybrid_quote(request_dict)` from `app.services.hybrid_quote`
     - Set state to "generated"
     - Include quote in response
  8. Elif ready but user hasn't asked to generate:
     - Set state to "ready"
     - Reply summarizes extracted fields and asks "Ready to generate the quote?" / "Pret a generer le devis?"
  9. Else (not ready):
     - Set state to "clarifying" or "extracting" based on what's missing
  10. Get context-aware suggestions via `get_suggestions()`
  11. Update session with new messages, fields, state
  12. Return ChatMessageResponse

- `POST /chat/reset` endpoint:
  ```python
  @router.post("/reset")
  async def reset_session(session_id: str):
  ```
  - Clears session via `chat_session.clear_session(session_id)`
  - Returns `{"status": "ok", "message": "Session reset"}`

- `GET /chat/session/{session_id}` endpoint:
  - Returns current session state (messages, extracted_fields, state)
  - Returns 404 if session doesn't exist
  - Useful for page reload recovery

Error handling:
- Wrap LLM call in try/except — if LLM fails, return a helpful fallback message: "I'm having trouble understanding. Could you try rephrasing?" with state staying the same
- Wrap quote generation in try/except — if it fails, return error message in reply but keep extracted fields

**2. backend/app/main.py** — Register the chat router:

- Add import: `from app.routers import chat` (add to existing import line)
- Add: `app.include_router(chat.router)` after the submissions router include
  </action>
  <verify>
    Run: `cd backend && python -c "from app.routers.chat import router; print(f'Router prefix: {router.prefix}, routes: {len(router.routes)}')"` — Should show prefix "/chat" and 3 routes.
    Run: `cd backend && python -c "from app.main import app; routes = [r.path for r in app.routes]; assert '/chat/message' in routes or any('/chat' in r for r in routes); print('Chat router registered in main app')"` — Should confirm chat routes are registered.
  </verify>
  <done>
    POST /chat/message endpoint accepts natural language, extracts fields via LLM, manages session state, auto-generates quotes when ready, and returns structured responses with suggestions. POST /chat/reset clears sessions. GET /chat/session/{id} returns current state. Router is registered in main.py alongside existing routers.
  </done>
</task>

</tasks>

<verification>
1. All 5 new files exist and are importable without errors
2. POST /chat/message accepts `{"session_id": "test-123", "message": "1200 sqft bardeaux", "language": "fr"}` and returns a ChatMessageResponse
3. Session persists between multiple calls with the same session_id
4. When enough fields are extracted, a "ready" state is returned with a prompt to generate
5. Chat router is registered in main.py and appears in FastAPI's auto-generated docs
6. Error handling: LLM failure returns fallback message without crashing
</verification>

<success_criteria>
- Backend chat endpoint is fully functional with session management
- Field extraction works via GPT-4o-mini with structured JSON output
- Conversation flows naturally through greeting -> extracting -> clarifying -> ready -> generated states
- Quote generation integrates with existing hybrid_quote service
- No new environment variables or external dependencies required
</success_criteria>

<output>
After completion, create `.planning/phases/27-ai-chat-interface/27-01-SUMMARY.md`
</output>
