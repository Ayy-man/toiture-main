---
phase: 27-ai-chat-interface
plan: 04
type: execute
wave: 2
depends_on: ["27-02"]
files_modified:
  - frontend/src/components/chat/voice-input-button.tsx
  - frontend/src/app/manifest.ts
  - frontend/src/app/layout.tsx
  - frontend/src/components/chat/chat-input.tsx
  - frontend/src/app/(admin)/chat/page.tsx
autonomous: true

must_haves:
  truths:
    - "Voice input button appears on supported browsers (Chrome, Safari) and transcribes speech to text"
    - "PWA manifest enables 'Add to Home Screen' on mobile devices"
    - "Chat input integrates voice button alongside send button"
    - "Voice input defaults to fr-CA language matching UI language toggle"
  artifacts:
    - path: "frontend/src/components/chat/voice-input-button.tsx"
      provides: "Web Speech API voice input button with fr-CA/en-US support"
      contains: "VoiceInputButton"
    - path: "frontend/src/app/manifest.ts"
      provides: "PWA web app manifest for home screen installation"
      contains: "manifest"
  key_links:
    - from: "frontend/src/components/chat/chat-input.tsx"
      to: "frontend/src/components/chat/voice-input-button.tsx"
      via: "VoiceInputButton rendered inside ChatInput"
      pattern: "VoiceInputButton"
    - from: "frontend/src/app/layout.tsx"
      to: "frontend/src/app/manifest.ts"
      via: "Next.js auto-links manifest from metadata"
      pattern: "manifest"
---

<objective>
Add voice input support (Web Speech API) and PWA manifest for mobile installation. This plan enhances the chat experience for Steven's mobile use case — he can speak into his phone at the job site instead of typing, and install the app to his home screen for quick access.

Purpose: Steven is at job sites with gloves and limited typing ability. Voice input in Quebec French (fr-CA) lets him describe jobs hands-free. PWA allows one-tap access without opening Safari/Chrome.

Output: Working voice input button in chat, PWA manifest for home screen installation, iOS/Android meta tags for standalone mode.
</objective>

<execution_context>
@/Users/aymanbaig/.claude/get-shit-done/workflows/execute-plan.md
@/Users/aymanbaig/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/27-ai-chat-interface/DESIGN.md
@.planning/phases/27-ai-chat-interface/27-RESEARCH.md
@.planning/phases/27-ai-chat-interface/27-02-SUMMARY.md
@frontend/src/app/layout.tsx
@frontend/src/components/chat/chat-input.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Voice input button with Web Speech API</name>
  <files>
    frontend/src/components/chat/voice-input-button.tsx
    frontend/src/components/chat/chat-input.tsx
  </files>
  <action>
**1. frontend/src/components/chat/voice-input-button.tsx** — Voice input component:
```
"use client";
```

Implementation based on research findings (Web Speech API):

- Add TypeScript declarations for webkitSpeechRecognition at top of file:
  ```typescript
  declare global {
    interface Window {
      webkitSpeechRecognition: new () => SpeechRecognition;
      SpeechRecognition: new () => SpeechRecognition;
    }
  }
  ```

- Props:
  - `onTranscript: (text: string) => void` — Called when speech is transcribed
  - `language?: "fr-CA" | "en-US"` — Language for recognition (default: "fr-CA")
  - `disabled?: boolean`

- State:
  - `isListening: boolean`
  - `isSupported: boolean` — Set in useEffect checking browser support

- useEffect on mount:
  - Check `typeof window !== "undefined" && ("webkitSpeechRecognition" in window || "SpeechRecognition" in window)`
  - Set isSupported accordingly

- toggleListening function:
  - If not listening: create new SpeechRecognition instance (webkitSpeechRecognition or SpeechRecognition), set `continuous = false`, `interimResults = false`, `lang = language`, set onresult/onerror/onend handlers, call `.start()`
  - If listening: call `.stop()` on current instance
  - onresult: extract transcript from `event.results[0][0].transcript`, call `onTranscript(transcript)`, set isListening=false
  - onerror: console.error the error, set isListening=false. If error is "not-allowed", the browser denied microphone access — could show a toast but for now just log
  - onend: set isListening=false (cleanup)

- Render:
  - If not supported, return null (graceful degradation — button simply doesn't appear on Firefox)
  - Return shadcn Button: `variant="ghost"`, `size="icon"`, onClick=toggleListening, disabled={disabled}
  - Icon: `Mic` when not listening (from lucide-react), `MicOff` when listening with `text-destructive` class for visual feedback
  - When listening, add a subtle pulse animation: `animate-pulse` on the button
  - Tooltip or aria-label: "Voice input" / "Stop listening" based on state
  - Add `aria-pressed={isListening}` for accessibility

**2. frontend/src/components/chat/chat-input.tsx** — Integrate voice button:

Update the ChatInput component (created in Plan 02) to accept and render VoiceInputButton:

- Add new optional prop: `onVoiceTranscript?: (text: string) => void`
- Add new optional prop: `voiceLanguage?: "fr-CA" | "en-US"`
- Import VoiceInputButton

- In the render, add VoiceInputButton between the textarea and send button:
  ```tsx
  <div className="flex items-end gap-2 p-2 border rounded-xl bg-background">
    <textarea ... />
    {onVoiceTranscript && (
      <VoiceInputButton
        onTranscript={onVoiceTranscript}
        language={voiceLanguage}
        disabled={disabled}
      />
    )}
    <Button onClick={handleSend} ...>
      <Send className="h-4 w-4" />
    </Button>
  </div>
  ```

- VoiceInputButton's onTranscript should append transcribed text to the current input value (not replace):
  - In ChatContainer (Plan 03), pass `onVoiceTranscript` that appends text to current input: `(text) => setInputValue(prev => prev ? prev + " " + text : text)`
  - The voice language should follow the UI locale: `locale === "fr" ? "fr-CA" : "en-US"`
  </action>
  <verify>
    Verify file exists: `ls frontend/src/components/chat/voice-input-button.tsx` — File exists.
    Verify integration: `grep "VoiceInputButton" frontend/src/components/chat/chat-input.tsx` — Import present.
    Run: `cd frontend && npx tsc --noEmit --pretty 2>&1 | grep -i "voice\|speech" | head -5` — No type errors related to voice components.
  </verify>
  <done>
    VoiceInputButton uses Web Speech API with fr-CA/en-US support. Renders only on supported browsers (Chrome, Safari). Integrates into ChatInput alongside send button. Transcribed text appends to current input. Visual pulse animation during recording.
  </done>
</task>

<task type="auto">
  <name>Task 2: PWA manifest and mobile meta tags</name>
  <files>
    frontend/src/app/manifest.ts
    frontend/src/app/layout.tsx
    frontend/src/app/(admin)/chat/page.tsx
  </files>
  <action>
**1. frontend/src/app/manifest.ts** — PWA web app manifest:

Create using Next.js 16 built-in manifest.ts convention:

```typescript
import type { MetadataRoute } from "next";

export default function manifest(): MetadataRoute.Manifest {
  return {
    name: "TOITURELV Cortex",
    short_name: "Cortex",
    description: "AI-powered roofing quote builder",
    start_url: "/chat",
    display: "standalone",
    background_color: "#ffffff",
    theme_color: "#1a1a1a",
    orientation: "portrait",
    icons: [
      {
        src: "/icon-192.png",
        sizes: "192x192",
        type: "image/png",
      },
      {
        src: "/icon-512.png",
        sizes: "512x512",
        type: "image/png",
      },
    ],
  };
}
```

Note: start_url is "/chat" because that's Steven's primary entry point on mobile.

For the icons: Create simple placeholder SVG icons that can be replaced later.
- Create `frontend/public/icon-192.png` and `frontend/public/icon-512.png` — use a simple programmatic approach: create a canvas-based icon OR use a single solid-color square with "LV" text as placeholder. Since we can't generate images directly, create a simple script that the executor can run, OR just note that icon files are needed (they can be placeholder PNGs for now — the manifest will still work without actual icons for testing, it just won't show a nice icon).
- Actually, the simplest approach: skip icon generation (icons are optional for manifest to work). The manifest will function for "Add to Home Screen" without icons — the browser uses a screenshot. Note in the plan summary that real icons should be added by the design team.

**2. frontend/src/app/layout.tsx** — Add mobile meta tags:

Read the existing layout.tsx and add/update metadata for PWA and mobile:

In the metadata export or `<head>` section, add:
- `<meta name="apple-mobile-web-app-capable" content="yes" />` — iOS standalone mode
- `<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />` — iOS status bar
- `<meta name="apple-mobile-web-app-title" content="Cortex" />` — iOS home screen name
- Ensure `viewport` meta has `maximum-scale=1, user-scalable=no` to prevent zoom on input focus (common iOS chat app pattern)
- Add `<meta name="mobile-web-app-capable" content="yes" />` for Android

If layout.tsx uses Next.js metadata export, add these via the `other` field:
```typescript
export const metadata: Metadata = {
  // ... existing fields ...
  other: {
    "apple-mobile-web-app-capable": "yes",
    "apple-mobile-web-app-status-bar-style": "black-translucent",
    "apple-mobile-web-app-title": "Cortex",
    "mobile-web-app-capable": "yes",
  },
};
```

If layout.tsx uses a `<head>` tag directly, add the meta tags there.

**3. frontend/src/app/(admin)/chat/page.tsx** — Add chat-specific mobile optimizations:

Update the chat page to add mobile-specific viewport handling:
- Add CSS to handle iOS safe areas: `padding-bottom: env(safe-area-inset-bottom)` on the chat container
- Ensure the page doesn't scroll behind the input when keyboard opens (the ChatContainer's flex layout should handle this naturally)
- Set page metadata if helpful:
  ```typescript
  export const metadata = {
    title: "Chat - Cortex",
  };
  ```
  Note: This won't work if the page is "use client". If so, use `document.title` in useEffect or skip — the breadcrumb in the admin layout shows the page name already.
  </action>
  <verify>
    Verify manifest: `ls frontend/src/app/manifest.ts` — File exists.
    Verify manifest content: `grep "start_url" frontend/src/app/manifest.ts` — Shows "/chat".
    Verify meta tags: `grep "apple-mobile-web-app" frontend/src/app/layout.tsx` — At least one match.
    Run: `cd frontend && npx tsc --noEmit --pretty 2>&1 | grep "manifest\|layout" | head -5` — No type errors.
  </verify>
  <done>
    PWA manifest.ts created with start_url="/chat" and standalone display mode. Mobile meta tags added to root layout for iOS and Android. Chat page handles safe area insets. Steven can "Add to Home Screen" on his iPhone for one-tap chat access. Voice input and PWA work together for a native-like mobile experience.
  </done>
</task>

</tasks>

<verification>
1. Voice input button appears in chat on Chrome/Safari (microphone icon)
2. Voice input button does NOT appear on Firefox (graceful degradation)
3. Tapping mic button starts listening (button pulses, icon changes to MicOff)
4. Speaking "mille deux cents pieds carres bardeaux" transcribes to text in input
5. Transcribed text appends to existing input content
6. Language toggle switches voice recognition between fr-CA and en-US
7. PWA manifest accessible at /_next/manifest.json or /manifest.webmanifest
8. "Add to Home Screen" prompt available on mobile browsers
9. Standalone mode (when installed) shows chat without browser chrome
10. iOS safe area insets respected (no content hidden behind home bar)
</verification>

<success_criteria>
- Voice input works on Chrome mobile (Android) and Safari (iOS)
- Web Speech API recognizes Quebec French (fr-CA) with reasonable accuracy
- PWA manifest enables standalone installation
- Mobile meta tags configure iOS and Android correctly
- No new npm dependencies required (Web Speech API is browser-native, manifest.ts is Next.js built-in)
</success_criteria>

<output>
After completion, create `.planning/phases/27-ai-chat-interface/27-04-SUMMARY.md`
</output>
