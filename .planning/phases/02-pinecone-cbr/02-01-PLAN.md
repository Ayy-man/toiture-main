---
phase: 02-pinecone-cbr
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/services/pinecone_cbr.py
  - backend/app/services/embeddings.py
  - backend/scripts/upload_embeddings.py
  - backend/requirements.txt
  - backend/.env.example
autonomous: true

must_haves:
  truths:
    - "Pinecone client initializes with API key"
    - "Embedding model generates 384-dim vectors"
    - "Upload script uploads all 8,132 vectors to Pinecone"
  artifacts:
    - path: "backend/app/services/pinecone_cbr.py"
      provides: "Pinecone connection and query operations"
      exports: ["init_pinecone", "close_pinecone", "query_similar_cases"]
    - path: "backend/app/services/embeddings.py"
      provides: "Query embedding generation"
      exports: ["load_embedding_model", "unload_embedding_model", "generate_query_embedding", "build_query_text"]
    - path: "backend/scripts/upload_embeddings.py"
      provides: "One-time embedding upload script"
      min_lines: 60
  key_links:
    - from: "backend/app/services/pinecone_cbr.py"
      to: "backend/app/config.py"
      via: "settings import"
      pattern: "from app\\.config import settings"
    - from: "backend/scripts/upload_embeddings.py"
      to: "cortex-data/cbr_embeddings.npz"
      via: "numpy load"
      pattern: "np\\.load.*cbr_embeddings"
---

<objective>
Create Pinecone CBR services and the embedding upload script.

Purpose: Establish the Pinecone infrastructure for case-based reasoning - connection management, query-time embedding generation, and one-time data upload.

Output: Working Pinecone services ready for integration and upload script that populates the index with 8,132 case embeddings.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-pinecone-cbr/02-RESEARCH.md

# Phase 1 structure (for service patterns)
@.planning/phases/01-fastapi-foundation/01-01-PLAN.md

# Data files for upload
@cortex-data/cbr_embeddings.npz (8132 embeddings, 384-dim)
@cortex-data/cbr_cases.json (case metadata)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Pinecone and embeddings services</name>
  <files>
    backend/app/services/pinecone_cbr.py
    backend/app/services/embeddings.py
    backend/requirements.txt
    backend/.env.example
  </files>
  <action>
First, update backend/requirements.txt to add Pinecone and sentence-transformers dependencies:
```
# Add to existing requirements.txt:
pinecone[grpc]>=5.0.0
sentence-transformers>=3.0.0
torch>=2.0.0
tqdm>=4.66.0
```

IMPORTANT: Use pinecone>=5.0.0 (NOT pinecone-client which is deprecated). For Python 3.9 compatibility, use pinecone>=5.0.0,<8.0.0 instead of >=8.0.0.

Update backend/.env.example to add Pinecone settings:
```
# Add to existing .env.example:
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_HOST=your_index_host_url
```

Update backend/app/config.py to add Pinecone settings:
- pinecone_api_key: str (required in production, can be empty string for tests)
- pinecone_index_host: str (required in production, can be empty string for tests)

Create backend/app/services/pinecone_cbr.py:
```python
"""Pinecone operations for Case-Based Reasoning."""

from pinecone.grpc import PineconeGRPC as Pinecone
from typing import List, Dict, Any, Optional
import logging
from app.config import settings

logger = logging.getLogger(__name__)

_pc: Pinecone = None
_index = None

def init_pinecone():
    """Initialize Pinecone client. Called from lifespan."""
    global _pc, _index
    if not settings.pinecone_api_key:
        logger.warning("Pinecone API key not set, skipping initialization")
        return
    logger.info("Connecting to Pinecone...")
    _pc = Pinecone(api_key=settings.pinecone_api_key)
    _index = _pc.Index(host=settings.pinecone_index_host)
    logger.info("Pinecone connected successfully")

def close_pinecone():
    """Cleanup on shutdown."""
    global _pc, _index
    _pc = None
    _index = None

def query_similar_cases(
    query_vector: List[float],
    top_k: int = 5,
    category_filter: Optional[str] = None,
    namespace: str = "cbr"
) -> List[Dict[str, Any]]:
    """Query Pinecone for similar historical cases."""
    if _index is None:
        logger.warning("Pinecone not initialized, returning empty results")
        return []

    filter_dict = None
    if category_filter:
        filter_dict = {"category": {"$eq": category_filter}}

    results = _index.query(
        vector=query_vector,
        top_k=top_k,
        include_metadata=True,
        namespace=namespace,
        filter=filter_dict
    )

    similar_cases = []
    for match in results.matches:
        similar_cases.append({
            "case_id": match.id,
            "similarity": round(float(match.score), 4),
            "category": match.metadata.get("category"),
            "sqft": match.metadata.get("sqft"),
            "total": match.metadata.get("total"),
            "per_sqft": match.metadata.get("per_sqft"),
            "year": match.metadata.get("year"),
        })

    return similar_cases
```

Create backend/app/services/embeddings.py:
```python
"""Query embedding generation using sentence-transformers."""

from sentence_transformers import SentenceTransformer
from typing import List, Optional
import logging

logger = logging.getLogger(__name__)

_model: SentenceTransformer = None

def load_embedding_model():
    """Load the embedding model at startup. Called from lifespan."""
    global _model
    logger.info("Loading sentence-transformers model...")
    _model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    logger.info("Embedding model loaded successfully")

def unload_embedding_model():
    """Cleanup on shutdown."""
    global _model
    _model = None

def generate_query_embedding(text: str) -> List[float]:
    """Generate 384-dim embedding for query text."""
    if _model is None:
        raise RuntimeError("Embedding model not loaded. Ensure lifespan started.")
    embedding = _model.encode(text, convert_to_numpy=True)
    return embedding.tolist()

def build_query_text(
    sqft: float,
    category: str,
    complexity: int,
    material_lines: Optional[int] = None,
    labor_lines: Optional[int] = None
) -> str:
    """Build query text from estimate inputs for semantic similarity."""
    parts = [f"Toiture {category}"]
    if sqft:
        parts.append(f"{sqft} pieds carres")
    parts.append(f"complexite {complexity}")
    if material_lines:
        parts.append(f"{material_lines} lignes materiaux")
    if labor_lines:
        parts.append(f"{labor_lines} lignes main-d'oeuvre")
    return ", ".join(parts)
```

IMPORTANT: The embedding model is CPU-bound. Functions should use regular def (not async def) to run in FastAPI's threadpool.
  </action>
  <verify>
    python -c "from backend.app.services.pinecone_cbr import init_pinecone, close_pinecone, query_similar_cases; print('pinecone_cbr OK')"
    python -c "from backend.app.services.embeddings import load_embedding_model, generate_query_embedding, build_query_text; print('embeddings OK')"
  </verify>
  <done>pinecone_cbr.py and embeddings.py services created with init/close/query functions</done>
</task>

<task type="auto">
  <name>Task 2: Create embedding upload script</name>
  <files>
    backend/scripts/__init__.py
    backend/scripts/upload_embeddings.py
  </files>
  <action>
Create backend/scripts/__init__.py (empty file).

Create backend/scripts/upload_embeddings.py:
```python
#!/usr/bin/env python3
"""One-time script to upload CBR embeddings to Pinecone.

Usage:
    export PINECONE_API_KEY=your_key
    export PINECONE_INDEX_NAME=toiturelv-cortex  # Optional, defaults to toiturelv-cortex
    python -m scripts.upload_embeddings
"""

import os
import json
import numpy as np
from pinecone.grpc import PineconeGRPC as Pinecone
from pinecone import ServerlessSpec
from tqdm import tqdm
from pathlib import Path

# Find data files relative to project root
PROJECT_ROOT = Path(__file__).parent.parent.parent
DATA_DIR = PROJECT_ROOT / "cortex-data"

def main():
    # Configuration
    api_key = os.environ.get("PINECONE_API_KEY")
    if not api_key:
        print("ERROR: PINECONE_API_KEY environment variable required")
        return

    index_name = os.environ.get("PINECONE_INDEX_NAME", "toiturelv-cortex")

    # Initialize Pinecone
    print("Connecting to Pinecone...")
    pc = Pinecone(api_key=api_key)

    # Create index if it doesn't exist
    existing_indexes = [idx.name for idx in pc.list_indexes()]
    if index_name not in existing_indexes:
        print(f"Creating index '{index_name}'...")
        pc.create_index(
            name=index_name,
            dimension=384,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1")
        )
        print("Index created. Waiting for it to be ready...")
    else:
        print(f"Index '{index_name}' already exists")

    # Get index host and connect
    index_info = pc.describe_index(index_name)
    index = pc.Index(host=index_info.host)

    # Store host URL for user to add to .env
    print(f"\n*** IMPORTANT: Add this to your .env file ***")
    print(f"PINECONE_INDEX_HOST={index_info.host}")
    print()

    # Load embeddings
    print("Loading embeddings from cortex-data/cbr_embeddings.npz...")
    embeddings_path = DATA_DIR / "cbr_embeddings.npz"
    data = np.load(embeddings_path)
    case_ids = data["case_ids"]
    embeddings = data["embeddings"]
    print(f"Loaded {len(case_ids)} embeddings ({embeddings.shape[1]} dimensions)")

    # Load case metadata
    print("Loading case metadata from cortex-data/cbr_cases.json...")
    cases_path = DATA_DIR / "cbr_cases.json"
    with open(cases_path) as f:
        cases_list = json.load(f)
    cases = {str(c["case_id"]): c for c in cases_list}
    print(f"Loaded {len(cases)} cases")

    # Prepare vectors
    print("Preparing vectors with metadata...")
    vectors = []
    for case_id, embedding in zip(case_ids, embeddings):
        case = cases.get(str(case_id), {})
        features = case.get("features", {})
        pricing = case.get("pricing", {})

        # Flatten metadata for Pinecone (max 40KB per record)
        # Remove None values (Pinecone doesn't accept null)
        metadata = {
            "case_id": str(case_id),
            "year": case.get("year"),
            "category": features.get("category", "Unknown"),
            "sqft": features.get("sqft"),
            "total": pricing.get("total"),
            "per_sqft": pricing.get("per_sqft"),
            "material_sell": pricing.get("material_sell"),
            "labor_sell": pricing.get("labor_sell"),
            "complexity_score": features.get("complexity_score"),
        }
        metadata = {k: v for k, v in metadata.items() if v is not None}

        vectors.append({
            "id": str(case_id),
            "values": embedding.tolist(),
            "metadata": metadata
        })

    # Upsert in batches
    batch_size = 500  # Optimal for 384-dim vectors
    print(f"Uploading {len(vectors)} vectors in batches of {batch_size}...")
    for i in tqdm(range(0, len(vectors), batch_size), desc="Uploading"):
        batch = vectors[i:i + batch_size]
        index.upsert(vectors=batch, namespace="cbr")

    # Verify
    print("\nVerifying upload...")
    stats = index.describe_index_stats()
    namespace_count = stats.namespaces.get("cbr", {}).vector_count if stats.namespaces else 0
    print(f"Index stats: {stats}")
    print(f"\nUpload complete! Vectors in 'cbr' namespace: {namespace_count}")

    if namespace_count == len(vectors):
        print("SUCCESS: All vectors uploaded!")
    else:
        print(f"WARNING: Expected {len(vectors)}, got {namespace_count}")


if __name__ == "__main__":
    main()
```

IMPORTANT considerations:
- Script uses Path for cross-platform file paths
- Finds cortex-data relative to project root (not CWD)
- Filters out None values from metadata (Pinecone requirement)
- Uses namespace "cbr" for organization
- Prints the PINECONE_INDEX_HOST URL for user to copy to .env
- Batch size 500 is optimal for 384-dim vectors
  </action>
  <verify>
    python -c "from backend.scripts.upload_embeddings import main; print('upload script imports OK')"
    head -30 backend/scripts/upload_embeddings.py  # Verify shebang and docstring
  </verify>
  <done>Upload script created that reads cbr_embeddings.npz and cbr_cases.json, upserts to Pinecone in batches</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Service modules import without errors:
   python -c "from backend.app.services.pinecone_cbr import init_pinecone, query_similar_cases"
   python -c "from backend.app.services.embeddings import load_embedding_model, generate_query_embedding"

2. Upload script imports and can find data files:
   python -c "from backend.scripts.upload_embeddings import main, DATA_DIR; print(DATA_DIR)"

3. requirements.txt contains Pinecone dependencies:
   grep pinecone backend/requirements.txt

4. .env.example has Pinecone variables:
   grep PINECONE backend/.env.example
</verification>

<success_criteria>
- pinecone_cbr.py has init_pinecone(), close_pinecone(), query_similar_cases()
- embeddings.py has load_embedding_model(), unload_embedding_model(), generate_query_embedding(), build_query_text()
- upload_embeddings.py reads cbr_embeddings.npz and cbr_cases.json
- Upload script handles None values in metadata
- Upload script prints PINECONE_INDEX_HOST for user
- All modules import without errors
</success_criteria>

<output>
After completion, create `.planning/phases/02-pinecone-cbr/02-01-SUMMARY.md`
</output>
