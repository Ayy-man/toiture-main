---
phase: 10-material-id-prediction
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - cortex-data/train_material_model.py
  - cortex-data/models/material_selector.pkl
  - cortex-data/models/material_binarizer.pkl
  - cortex-data/models/quantity_regressors.pkl
  - cortex-data/models/co_occurrence_rules.json
  - cortex-data/models/feature_triggers.json
  - cortex-data/models/material_prices.json
autonomous: true

must_haves:
  truths:
    - "Training script runs to completion without errors"
    - "Material ID selection F1 micro >= 70%"
    - "Quantity MAPE <= 30% for top 20 materials"
    - "Co-occurrence rules extracted with P > 0.7"
    - "Feature triggers extracted for chimney/skylight"
  artifacts:
    - path: "cortex-data/train_material_model.py"
      provides: "Training script"
      min_lines: 200
    - path: "cortex-data/models/material_selector.pkl"
      provides: "OneVsRestClassifier model"
    - path: "cortex-data/models/material_binarizer.pkl"
      provides: "MultiLabelBinarizer for encoding"
    - path: "cortex-data/models/quantity_regressors.pkl"
      provides: "Dict of per-material regressors"
    - path: "cortex-data/models/co_occurrence_rules.json"
      provides: "Association rules with confidence >= 0.7"
    - path: "cortex-data/models/feature_triggers.json"
      provides: "Chimney and skylight material mappings"
    - path: "cortex-data/models/material_prices.json"
      provides: "Median unit_price per material_id"
  key_links:
    - from: "train_material_model.py"
      to: "material_training_data.json"
      via: "json.load"
      pattern: "material_training_data\\.json"
    - from: "train_material_model.py"
      to: "cortex-data/models/"
      via: "joblib.dump and json.dump"
      pattern: "joblib\\.dump|json\\.dump"
---

<objective>
Train multi-label material ID classifier and per-material quantity regressors

Purpose: Enable prediction of which materials (and quantities) are needed for roofing jobs
Output: Trained models and extracted rules saved to cortex-data/models/
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/10-material-id-prediction-model-training/10-RESEARCH.md

# Training data
@cortex-data/material_training_data.json (7,433 samples)

# Existing patterns to follow
@backend/app/services/predictor.py (lazy loading pattern)
@cortex-data/train_cortex_v4.py (training script patterns)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create training script with data preprocessing</name>
  <files>cortex-data/train_material_model.py</files>
  <action>
Create training script that:

1. **Load and preprocess data:**
   - Load material_training_data.json (7,433 samples)
   - Extract features: sqft, complexity_score, quoted_total, has_chimney, has_skylights, material_lines, labor_lines, has_subs, job_category
   - Encode job_category with LabelEncoder (save encoder)
   - Filter materials: remove entries with quantity=0, keep material_ids with >= 50 samples
   - Build y as list of material_id sets per sample
   - Split 80/20 train/test with stratification if possible (use iterative stratification from skmultilearn if available, else random)

2. **Train material selector:**
   - Use MultiLabelBinarizer to encode material_id sets
   - Train OneVsRestClassifier with RandomForestClassifier(n_estimators=100, class_weight='balanced', n_jobs=-1, random_state=42)
   - Compute F1 micro on test set (requirement: >= 70%)
   - If F1 < 70%, try tuning threshold from 0.5 to 0.3 using predict_proba

3. **Train quantity regressors:**
   - For each material_id with >= 50 samples in training set:
     - Filter samples where this material_id appears
     - Extract quantity as target, features as X
     - Train GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42)
     - Store in dict: {material_id: trained_model}
   - Compute MAPE for top 20 most frequent materials (requirement: <= 30%)

4. **Extract co-occurrence rules:**
   - Count material pair co-occurrences across all samples
   - For each pair (A, B): compute confidence = count(A and B) / count(A)
   - Keep rules where confidence >= 0.7 and support >= 50
   - Save as list of {antecedent, consequent, confidence, support}

5. **Extract feature triggers:**
   - For has_chimney=True samples, find materials that appear in >= 70% of those samples but < 30% of has_chimney=False samples
   - Same for has_skylights
   - Save as {chimney_materials: [...], skylight_materials: [...]}

6. **Compute price lookup:**
   - For each material_id, compute median unit_cost from all samples
   - Save as {material_id: median_price}

7. **Save all artifacts to cortex-data/models/:**
   - material_selector.pkl (the trained OneVsRestClassifier)
   - material_binarizer.pkl (the fitted MultiLabelBinarizer)
   - quantity_regressors.pkl (dict of {material_id: regressor})
   - co_occurrence_rules.json
   - feature_triggers.json
   - material_prices.json
   - category_encoder_material.pkl (LabelEncoder for job_category)

8. **Print evaluation metrics:**
   - F1 micro, F1 macro
   - MAPE for top 20 materials (individual + average)
   - Number of co-occurrence rules extracted
   - Number of chimney/skylight triggered materials

Follow patterns from cortex-data/train_cortex_v4.py for structure and joblib usage.
  </action>
  <verify>
Run the training script:
```bash
cd /Users/aymanbaig/Desktop/Toiture-P1/cortex-data && python train_material_model.py
```
Verify:
- Script completes without errors
- F1 micro printed >= 0.70
- MAPE printed <= 30%
- All 7 artifact files exist in cortex-data/models/
  </verify>
  <done>
- train_material_model.py runs successfully
- F1 micro >= 70% achieved
- MAPE <= 30% for top 20 materials
- All model artifacts saved to cortex-data/models/
  </done>
</task>

<task type="auto">
  <name>Task 2: Copy model artifacts to backend</name>
  <files>backend/app/models/</files>
  <action>
Copy the trained model artifacts from cortex-data/models/ to backend/app/models/:

```bash
cp cortex-data/models/material_selector.pkl backend/app/models/
cp cortex-data/models/material_binarizer.pkl backend/app/models/
cp cortex-data/models/quantity_regressors.pkl backend/app/models/
cp cortex-data/models/co_occurrence_rules.json backend/app/models/
cp cortex-data/models/feature_triggers.json backend/app/models/
cp cortex-data/models/material_prices.json backend/app/models/
cp cortex-data/models/category_encoder_material.pkl backend/app/models/
```

This makes models available to the FastAPI backend at runtime.
  </action>
  <verify>
```bash
ls -la backend/app/models/material*.* backend/app/models/co_occurrence* backend/app/models/feature_triggers* backend/app/models/category_encoder_material*
```
Verify all 7 files exist with non-zero size.
  </verify>
  <done>
- All model artifacts copied to backend/app/models/
- Files have non-zero size
  </done>
</task>

</tasks>

<verification>
1. Training script runs to completion: `cd cortex-data && python train_material_model.py`
2. F1 micro score printed >= 0.70
3. MAPE score printed <= 30%
4. Model files exist:
   - cortex-data/models/material_selector.pkl
   - cortex-data/models/material_binarizer.pkl
   - cortex-data/models/quantity_regressors.pkl
   - cortex-data/models/co_occurrence_rules.json
   - cortex-data/models/feature_triggers.json
   - cortex-data/models/material_prices.json
5. Same files copied to backend/app/models/
</verification>

<success_criteria>
- Training script produces F1 micro >= 70%
- Quantity MAPE <= 30% for top 20 materials
- All 7 model artifacts saved and copied to backend
- Co-occurrence rules extracted (expecting ~495 rules with P > 0.7)
- Feature triggers extracted for chimney and skylight materials
</success_criteria>

<output>
After completion, create `.planning/phases/10-material-id-prediction-model-training/10-01-SUMMARY.md`
</output>
