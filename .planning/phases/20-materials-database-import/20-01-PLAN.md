---
phase: 20-materials-database-import
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/scripts/import_materials.py
  - backend/app/scripts/detect_duplicates.py
  - backend/app/scripts/create_materials_table.sql
  - backend/requirements.txt
autonomous: true

must_haves:
  truths:
    - "Materials table exists in Supabase with all required columns"
    - "813+ clean material items imported with review_status='approved'"
    - "259 flagged items imported with review_status='flagged'"
    - "37 labor items filtered out or marked as item_type='labor'"
    - "Duplicate and near-duplicate materials identified and flagged"
  artifacts:
    - path: "backend/app/scripts/create_materials_table.sql"
      provides: "SQL DDL for materials table with indexes and pg_trgm"
      contains: "CREATE TABLE materials"
    - path: "backend/app/scripts/import_materials.py"
      provides: "CSV cleaning, validation, and Supabase batch import"
      contains: "def import_materials"
    - path: "backend/app/scripts/detect_duplicates.py"
      provides: "RapidFuzz fuzzy matching for deduplication"
      contains: "rapidfuzz"
  key_links:
    - from: "backend/app/scripts/import_materials.py"
      to: "Supabase materials table"
      via: "supabase.from_('materials').insert()"
      pattern: "supabase.*materials.*insert"
    - from: "backend/app/scripts/detect_duplicates.py"
      to: "Supabase materials table"
      via: "supabase.from_('materials').update()"
      pattern: "review_status.*duplicate"
---

<objective>
Create the Supabase materials table, import Laurent's 1,152-item CSV with data cleaning and validation, and run deduplication analysis.

Purpose: Foundation data layer -- all subsequent plans (search API, material selector UI) depend on materials being in the database.
Output: Materials table populated with ~813 clean items + ~259 flagged items, deduplication report generated.
</objective>

<execution_context>
@/Users/aymanbaig/.claude/get-shit-done/workflows/execute-plan.md
@/Users/aymanbaig/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-materials-database-import/20-RESEARCH.md
@backend/app/services/supabase_client.py
@cortex-data/LV Material List.csv
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create materials table SQL and CSV import script</name>
  <files>
    backend/app/scripts/create_materials_table.sql
    backend/app/scripts/import_materials.py
    backend/requirements.txt
  </files>
  <action>
    1. Add `pandas>=2.0.0` and `rapidfuzz>=3.0.0` to `backend/requirements.txt` (append after existing deps).

    2. Create `backend/app/scripts/create_materials_table.sql` with:
       - `CREATE EXTENSION IF NOT EXISTS pg_trgm;` (for fuzzy text search)
       - `CREATE TABLE IF NOT EXISTS materials` with columns:
         - id BIGSERIAL PRIMARY KEY
         - code VARCHAR(100) (nullable -- supplier code from CODE column)
         - name TEXT NOT NULL
         - cost DECIMAL(10,2) (nullable -- 43 items missing)
         - sell_price DECIMAL(10,2) (nullable -- 42 items missing)
         - unit VARCHAR(50) NOT NULL
         - category VARCHAR(100) (nullable -- 231 items missing)
         - supplier VARCHAR(200) (nullable)
         - note TEXT (nullable)
         - area_sqft DECIMAL(10,2) DEFAULT 0
         - length_ft DECIMAL(10,2) DEFAULT 0
         - width_ft DECIMAL(10,2) DEFAULT 0
         - thickness_ft DECIMAL(10,2) DEFAULT 0
         - item_type VARCHAR(20) DEFAULT 'material' (values: 'material', 'labor')
         - ml_material_id INTEGER (nullable -- for future ML model mapping)
         - review_status VARCHAR(20) DEFAULT 'approved' (values: 'approved', 'flagged', 'duplicate')
         - created_at TIMESTAMPTZ DEFAULT NOW()
         - updated_at TIMESTAMPTZ DEFAULT NOW()
       - Indexes:
         - `CREATE INDEX idx_materials_category ON materials(category);`
         - `CREATE INDEX idx_materials_name_trgm ON materials USING gin(name gin_trgm_ops);`
         - `CREATE INDEX idx_materials_code ON materials(code) WHERE code IS NOT NULL;`
         - `CREATE INDEX idx_materials_review ON materials(review_status);`
         - `CREATE INDEX idx_materials_item_type ON materials(item_type);`

    3. Create `backend/app/scripts/import_materials.py`:
       - Standalone script (runs with `python -m app.scripts.import_materials` from backend/)
       - Uses environment variables SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY (load from .env or os.environ)
       - Reads CSV from path argument or defaults to `../../cortex-data/LV Material List.csv`
       - Uses pandas for CSV parsing with `encoding='utf-8-sig'` (handles BOM)
       - Column mapping: NAME->name, COST->cost, SELL->sell_price, UNIT->unit, CATEGORY->category, SUPPLIER->supplier, CODE->code, NOTE->note, "S en pi2"->area_sqft, "Long en pi"->length_ft, "Larg en pi"->width_ft, "Ep en pi"->thickness_ft
       - Data cleaning pipeline:
         a. Strip whitespace from all string columns
         b. Convert COST and SELL to numeric (remove $ and comma, errors='coerce')
         c. Convert dimension columns to numeric (errors='coerce', fillna 0)
         d. Detect labor items: categories "Main d'oeuvre" and "Sous-traitant pose" -> set item_type='labor'
         e. Detect duplicates: df.duplicated(subset=['NAME'], keep='first')
         f. Classify completeness: has NAME + COST + SELL + UNIT + CATEGORY = complete
         g. Set review_status:
            - Complete + not duplicate + not labor -> 'approved'
            - Duplicate -> 'duplicate'
            - Incomplete or labor -> 'flagged'
       - Batch insert to Supabase in chunks of 500 (supabase client max is 1000 but 500 is safer)
       - Print summary stats: total imported, approved count, flagged count, duplicate count, labor count
       - Add `--dry-run` flag that prints stats without inserting
       - Add `__main__` block with argparse

    NOTE: The SQL file is for reference/documentation. The actual table should be created via Supabase Dashboard SQL Editor (run the SQL manually), since we cannot run DDL through the Supabase client library. The import script handles only the data insertion.
  </action>
  <verify>
    - `python -c "import pandas; import rapidfuzz; print('OK')"` succeeds (after pip install)
    - `python -m app.scripts.import_materials --dry-run` from backend/ prints stats without errors
    - SQL file exists and is valid SQL syntax
  </verify>
  <done>
    - import_materials.py runs in dry-run mode showing: ~1072 data rows parsed, ~813 approved, ~259 flagged, ~37 labor, ~4 duplicates
    - SQL DDL file ready for manual execution in Supabase Dashboard
    - pandas and rapidfuzz added to requirements.txt
  </done>
</task>

<task type="auto">
  <name>Task 2: Run import and create deduplication report</name>
  <files>
    backend/app/scripts/detect_duplicates.py
  </files>
  <action>
    1. Create `backend/app/scripts/detect_duplicates.py`:
       - Standalone script (runs with `python -m app.scripts.detect_duplicates` from backend/)
       - Loads all materials from Supabase where item_type='material'
       - Uses RapidFuzz `fuzz.token_sort_ratio` to compare all material name pairs
       - Threshold: 85 (similar enough to flag)
       - For each pair above threshold, print: similarity score, name1, name2, id1, id2
       - Group results by cluster (if A~B and B~C, group all three)
       - Output: Print report to stdout and optionally write to CSV file
       - Flag matches in database: update review_status='duplicate' for lower-ID duplicates (keep higher-ID as canonical if needed, but for now just flag both for manual review)
       - Add `--threshold` argument (default 85)
       - Add `--dry-run` flag that prints report without updating database
       - Uses environment variables SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY

    2. IMPORTANT: After creating both scripts, run the actual import:
       a. First, print the SQL from create_materials_table.sql and instruct the user to run it in Supabase Dashboard (checkpoint would be needed, but since we cannot automate Supabase DDL, include instructions as a print statement in the import script that checks if table exists first)
       b. Actually, the import script should attempt the insert -- if the table doesn't exist, Supabase will return an error. Add a check at the start of import_materials.py: query `supabase.from_('materials').select('id').limit(1)` -- if it fails, print "ERROR: materials table does not exist. Please run create_materials_table.sql in Supabase Dashboard first." and exit.
       c. Run `python -m app.scripts.import_materials` to actually import data (with SUPABASE env vars set)
       d. Run `python -m app.scripts.detect_duplicates --dry-run` to generate the report

    NOTE: If Supabase env vars are not available locally, the scripts should still be complete and correct. The actual import can happen when deployed or when env vars are configured. The dry-run mode validates the CSV parsing logic independently of the database.
  </action>
  <verify>
    - `python -m app.scripts.detect_duplicates --dry-run` runs without errors (even if Supabase not connected, it should handle gracefully)
    - Both scripts have `--help` output via argparse
    - Scripts handle missing SUPABASE env vars gracefully with clear error messages
  </verify>
  <done>
    - detect_duplicates.py runs and identifies near-duplicate material names
    - Import script includes table-existence check with clear error message
    - Both scripts have --dry-run and --help flags
    - If Supabase is configured: materials are imported, dedup report generated
    - If Supabase is not configured: scripts exit cleanly with instructions
  </done>
</task>

</tasks>

<verification>
1. `ls backend/app/scripts/` shows create_materials_table.sql, import_materials.py, detect_duplicates.py
2. `python -m app.scripts.import_materials --dry-run` (from backend/) parses CSV correctly
3. `python -m app.scripts.detect_duplicates --help` shows usage
4. `grep pandas backend/requirements.txt` finds the dependency
5. `grep rapidfuzz backend/requirements.txt` finds the dependency
</verification>

<success_criteria>
- Materials table SQL DDL created with all columns, indexes, and pg_trgm extension
- CSV import script handles: encoding (UTF-8 BOM), price cleaning, labor filtering, completeness checking, batch insert
- Deduplication script uses RapidFuzz with configurable threshold
- Both scripts have dry-run mode for testing without database
- Backend requirements updated with pandas and rapidfuzz
</success_criteria>

<output>
After completion, create `.planning/phases/20-materials-database-import/20-01-SUMMARY.md`
</output>
